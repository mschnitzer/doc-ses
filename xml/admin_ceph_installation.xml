<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<!-- Converted by suse-upgrade version 1.1 -->
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="cha.ceph.install">
 <title>Installation of Basic &ceph; Cluster</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:maintainer>tbazant@suse.com</dm:maintainer>
   <dm:status>editing</dm:status>
   <dm:deadline></dm:deadline>
   <dm:priority></dm:priority>
   <dm:translation></dm:translation>
   <dm:languages></dm:languages>
   <dm:release>SES2.1</dm:release>
  </dm:docmanager>
 </info>
 <para>
  This chapter outlines procedures to deploy the &ceph; cluster. Currently
  we support the following methods of deployment:
 </para>
 <itemizedlist>
  <listitem>
   <para>
    <xref linkend="ceph.install.ceph-deploy"/>
   </para>
  </listitem>
  <listitem>
   <para>
    <xref linkend="ceph.install.crowbar"/>
   </para>
  </listitem>
 </itemizedlist>
 <sect1 xml:id="ceph.install.ceph-deploy">
  <title>Deploying with <command>ceph-deploy</command></title>

  <para>
   <command>ceph-deploy</command> is a command line utility to ease the way
   you deploy &ceph; cluster in small scale setups.
  </para>

  <sect2 xml:id="ceph.install.ceph-deploy.layout">
   <title>&ceph; Layout</title>
   <para>
    For testing purposes, a minimal &ceph; cluster can be made to run on a
    single node. However, in a production setup we recommend using at least
    four nodes: one admin node and three cluster nodes, each running one
    monitor daemon and some number of object storage daemons (OSDs).
   </para>
   <figure>
    <title>Minimal &ceph; Setup</title>
    <mediaobject>
     <imageobject role="fo">
      <imagedata fileref="ceph_minimal.png" width="60%" format="PNG"/>
     </imageobject>
     <imageobject role="html">
      <imagedata fileref="ceph_minimal.png" width="60%" format="PNG"/>
     </imageobject>
    </mediaobject>
   </figure>
   <tip>
    <para>
     Although &ceph; nodes can be virtual machines, real hardware is
     strongly recommended for the production environment.
    </para>
   </tip>
  </sect2>

  <sect2 xml:id="ceph.install.ceph-deploy.network">
   <title>Network Recommendations</title>
   <para>
    The network environment where you intend to run &ceph; should ideally be
    a bonded set of at least two network interfaces that is logically split
    into a public part and trusted internal part using VLANs. The bonding
    mode is recommended to be 802.3ad when possible to provide maximum
    bandwidth and resiliency.
   </para>
   <para>
    The public VLAN serves for providing the service to the customers, the
    internal part provides for the authenticated &ceph; network
    communication. The main reason is that although &ceph; authentication
    and protection against attacks once secret keys are in place, the
    messages used to configure these keys may be transferred open and are
    vulnerable.
   </para>
   <tip>
    <title>Nodes Configured via DHCP</title>
    <para>
     If your storage nodes are configured via DHCP, the default timeouts may
     not be sufficient for the network to be configured correctly before the
     various &ceph; daemons start. If this happens, the &ceph; MONs and OSDs
     will not start correctly (running <command>systemctl status
     ceph\*</command> will result in "unable to bind" errors), and Calamari
     may be unable to display graphs. To avoid this issue, we recommend
     increasing the DHCP client timeout to at least 30 seconds on each node
     in your storage cluster. This can be done by changing the following
     settings on each node:
    </para>
    <para>
     In <filename>/etc/sysconfig/network/dhcp</filename> set
    </para>
<screen>DHCLIENT_WAIT_AT_BOOT="30"</screen>
    <para>
     In <filename>/etc/sysconfig/network/config</filename> set
    </para>
<screen>WAIT_FOR_INTERFACES="60"</screen>
   </tip>
  </sect2>

  <sect2 xml:id="ceph.install.ceph-deploy.eachnode">
   <title>Preparing Each &ceph; Node</title>
   <para>
    Before deploying the &ceph; cluster, apply the following steps for each
    &ceph; node as &rootuser;:
   </para>
   <procedure>
    <step>
     <para>
      Install &sle; 12 SP1 and add the &storage; extension. It provides a
      software repository with the software needed to run &ceph;. For more
      information on the extension installation, see
      <link xlink:href="https://www.suse.com/documentation/sles-12/book_sle_deployment/data/cha_add-ons.html"/>.
     </para>
     <important>
      <title>&ceph; Runs as <systemitem>ceph</systemitem> User and Group</title>
      <para>
       In &storage; version 3.0, &ceph; runs as
       <systemitem>ceph</systemitem> user and group, both with default
       UID/GID 167. If UID/GID 167 is not available across your cluster, you
       may need to create the <systemitem>ceph</systemitem> user and group
       manually. For more information, see
       <xref linkend="ceph.upgrade.2.1to3.ceph_uid"/>.
      </para>
     </important>
    </step>
    <step>
     <para>
      Check the firewall status
     </para>
<screen>sudo /sbin/SuSEfirewall2 status</screen>
     <para>
      and if it is on, either turn it off with
     </para>
<screen>sudo /sbin/SuSEfirewall2 off</screen>
     <para>
      or, if you want to keep it on, enable the appropriate set of ports.
      You can find detailed information in
      <xref linkend="storage.bp.net.firewall"/>.
     </para>
    </step>
    <step>
     <para>
      Make sure that network settings are correct: each &ceph; node needs to
      route to all other &ceph; nodes, and each &ceph; node needs to resolve
      all other &ceph; nodes by their short host names (without the domain
      suffix). If these two conditions are not met, &ceph; fails.
     </para>
    </step>
    <step>
     <para>
      Install and set up NTP&mdash;the time synchronization tool. We
      strongly recommend using NTP within the &ceph; cluster. The reason is
      that &ceph; daemons pass critical messages to each other, which must
      be processed before daemons reach a timeout threshold. If the clocks
      in &ceph; monitors are not synchronized, it can lead to a number of
      anomalies, such as daemons ignoring received messages.
     </para>
     <para>
      Even though clock drift may still be noticeable with NTP, it is not
      yet harmful.
     </para>
     <para>
      To install NTP, run the following:
     </para>
<screen>sudo zypper in ntp yast2-ntp-client</screen>
     <para>
      To configure NTP, go to <menuchoice><guimenu>&yast;</guimenu>
      <guimenu>Network Services</guimenu> <guimenu>NTP
      Configuration</guimenu></menuchoice>. Make sure to enable the NTP
      service (<command>systemctl enable ntpd.service &amp;&amp; systemctl
      start ntpd.service</command>). Find more detailed information on NTP
      in the
      <link xlink:href="https://www.suse.com/documentation/sles-12/book_sle_admin/data/cha_netz_xntp.html">
      SLES Administration Guide</link>.
     </para>
    </step>
    <step>
     <para>
      Install SSH server. &ceph; uses SSH to log in to all cluster nodes.
      Make sure SSH is installed (<command>zypper in openssh</command>) and
      enabled (<command>systemctl enable sshd.service &amp;&amp; systemctl
      start sshd.service</command>).
     </para>
    </step>
    <step>
     <para>
      Add a &cephuser; user account, and set password for it. The admin node
      will log in to &ceph; nodes as this particular &cephuser; user .
     </para>
<screen>useradd -m &cephuser; &amp;&amp; passwd &cephuser;</screen>
    </step>
    <step>
     <para>
      The admin node needs to have passwordless SSH access to all &ceph;
      nodes. When <command>ceph-deploy</command> logs in to a &ceph; node as
      a &cephuser; user, this user must have passwordless
      <command>sudo</command> privileges.
     </para>
     <para>
      Edit the <filename>/etc/sudoers</filename> file (with
      <command>visudo</command>) and add the following line to add the
      <command>sudo</command> command for the &cephuser; user:
     </para>
<screen>&cephuser.plain; ALL = (root) NOPASSWD:ALL</screen>
     <tip>
      <title>Disable <literal>requiretty</literal></title>
      <para>
       You may receive an error while trying to execute
       <command>ceph-deploy</command> commands. If
       <literal>requiretty</literal> is set by default, disable it by
       executing <command>sudo visudo</command> and locate the
       <literal>Defaults requiretty</literal> setting. Change it to<literal>
       Defaults:&cephuser.plain; !requiretty</literal> to ensure that
       <command>ceph-deploy</command> can connect using the &cephuser; user
       and execute commands with <command>sudo</command>.
      </para>
     </tip>
    </step>
    <step>
     <para>
      On the admin node, become the &cephuser; user, and enable passwordless
      SSH access to all other &ceph; nodes:
     </para>
<screen>su - &cephuser;
ssh-keygen</screen>
     <para>
      You will be asked several questions. Leave the values at their
      defaults, and the passphrase empty.
     </para>
     <para>
      Copy the key to each &ceph; node:
     </para>
<screen>ssh-copy-id &cephuser;@<replaceable>node1</replaceable>
ssh-copy-id &cephuser;@<replaceable>node2</replaceable>
ssh-copy-id &cephuser;@<replaceable>node3</replaceable></screen>
     <tip>
      <title>Running <command>ceph-deploy</command> from a Different User Account Than &cephuser;</title>
      <para>
       It is possible to run the <command>ceph-deploy</command> command even
       if you are logged in as a different user than &cephuser;. For this
       purpose, you need to set up an SSH alias in your
       <filename>~/.ssh/config</filename> file:
      </para>
<screen>[...]
Host ceph-node1
  Hostname ceph-node1
  User &cephuser;</screen>
      <para>
       After this change, <command>ssh ceph-node1</command> automatically
       uses the &cephuser; user to log in.
      </para>
     </tip>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="ceph.install.ceph-deploy.purge">
   <title>Cleaning Previous &ceph; Environment</title>
   <para>
    If at any point during the &ceph; deployment you run into trouble and
    need to start over, or you want to make sure that any previous &ceph;
    configuration is removed, execute the following commands as &cephuser;
    user to purge the previous &ceph; configuration.
   </para>
   <warning>
    <para>
     Be aware that <emphasis>purging</emphasis> previous &ceph; installation
     destroys stored data and access settings.
    </para>
   </warning>
<screen>&prompt.cephuser;ceph-deploy purge <replaceable>node1</replaceable> <replaceable>node2</replaceable> <replaceable>node3</replaceable>
&prompt.cephuser;ceph-deploy purgedata <replaceable>node1</replaceable> <replaceable>node2</replaceable> <replaceable>node3</replaceable>
&prompt.cephuser;ceph-deploy forgetkeys</screen>
  </sect2>

  <sect2 xml:id="ceph.install.ceph-deploy.cephdeploy">
   <title>Running <command>ceph-deploy</command></title>
   <para>
    After you prepared each &ceph; node as described in
    <xref linkend="ceph.install.ceph-deploy.eachnode"/>, you are ready to
    deploy &ceph; from the admin node with <command>ceph-deploy</command>.
    Note that <command>ceph-deploy</command> will not successfully install
    an OSD on disks that have been previously used, unless you first 'zap'
    them. Be aware that 'zapping' erases the entire disk content:
   </para>
<screen>&prompt.cephuser;ceph-deploy disk zap <replaceable>node:vdb</replaceable></screen>
   <procedure>
    <step>
     <para>
      Install <command>ceph</command> and <command>ceph-deploy</command>:
     </para>
<screen>sudo zypper in ceph ceph-deploy</screen>
    </step>
    <step>
     <para>
      Disable IPv6. Open <filename>/etc/sysctl.conf</filename>, edit the
      following lines, and reboot the admin node:
     </para>
<screen>net.ipv6.conf.all.disable_ipv6 = 1
net.ipv6.conf.default.disable_ipv6 = 1
net.ipv6.conf.lo.disable_ipv6 = 1</screen>
    </step>
    <step>
     <para>
      Because it is not recommended to run <command>ceph-deploy</command> as
      &rootuser;, become the &cephuser; user:
     </para>
<screen>su - &cephuser;</screen>
    </step>
    <step>
     <para>
      Run <command>ceph-deploy</command> to install &ceph; on each node:
     </para>
<screen>&prompt.cephuser;ceph-deploy install <replaceable>node1</replaceable> <replaceable>node2</replaceable> <replaceable>node3</replaceable></screen>
     <tip>
      <para>
       <command>ceph-deploy</command> creates important files in the
       directory where you run it from. It is best to run
       <command>ceph-deploy</command> in an empty directory.
      </para>
     </tip>
    </step>
    <step>
     <para>
      Set up the monitor nodes. Create keys and local configuration. The
      keys are used to authenticate and protect the communication between
      &ceph; nodes.
     </para>
<screen>&prompt.cephuser;ceph-deploy new <replaceable>node1</replaceable> <replaceable>node2</replaceable> <replaceable>node3</replaceable></screen>
     <para>
      During this step, <command>ceph-deploy</command> creates local
      configuration files. It is recommended to inspect the configuration
      files in the current directory.
     </para>
     <tip>
      <title>Monitor Nodes on Different Subnets</title>
      <para>
       If the monitor nodes are not in the same subnet, you need to modify
       the <filename>ceph.conf</filename> in the current directory. For
       example, if the nodes have IP addresses
      </para>
<screen>10.121.9.186
10.121.10.186
10.121.11.186</screen>
      <para>
       add the following line to the global section of
       <filename>ceph.conf</filename>:
      </para>
<screen>public network = 10.121.0.0/16</screen>
      <para>
       Since you are likely to experience problems with IPv6 networking,
       consider modifying the IPv6 mon_host settings, as in the following
       example:
      </para>
<screen>mon_host = [2620:...10:121:9:186,2620:...10:121:10:186,2620:...10:121:11:186]</screen>
      <para>
       into its IPv4 equivalent:
      </para>
<screen>mon_host = 10.121.9.186, 10.121.10.186, 10.121.11.186</screen>
     </tip>
    </step>
    <step>
     <para>
      Create the initial monitor service on already created monitor nodes:
     </para>
<screen>&prompt.cephuser;ceph-deploy mon create-initial</screen>
    </step>
    <step>
     <para>
      Any node from which you need to run &ceph; command line tools needs a
      copy of the admin keyring. To copy the admin keyring to a node or set
      of nodes, run
     </para>
<screen>ceph-deploy admin node1 [node2] [node2]</screen>
     <important>
      <para>
       Because the <literal>client.admin</literal>'s keyring file is
       readable by &rootuser; only, you need to use <command>sudo</command>
       when running the <command>ceph</command> command.
      </para>
     </important>
    </step>
    <step>
     <para>
      Check the firewall status
     </para>
<screen>sudo /sbin/SuSEfirewall2 status</screen>
     <para>
      and if it is off, check its configuration and turn it on with
     </para>
<screen>sudo /sbin/SuSEfirewall2 on</screen>
     <para>
      You can find detailed information in
      <xref linkend="storage.bp.net.firewall"/>.
     </para>
    </step>
    <step>
     <para>
      Create OSD daemons. Although you can use a directory as a storage, we
      recommend to create a separate disk dedicated to a &ceph; node. To
      find out the name of the disk device, run
     </para>
<screen>cat /proc/partitions
major minor  #blocks  name

 254        0   12582912 vda
 254        1    1532928 vda1
 254        2   11048960 vda2
  11        0    2831360 sr0
 254       16    4194304 vdb</screen>
     <para>
      In our case the <systemitem>vdb</systemitem> disk has no partitions,
      so it is most likely our newly created disk.
     </para>
     <para>
      Now set up the disk for &ceph;:
     </para>
<screen>&prompt.cephuser;ceph-deploy osd prepare <replaceable>node:vdb</replaceable></screen>
     <tip>
      <para>
       If the disk was already used before, add the <option>--zap</option>
       option.
      </para>
<screen>&prompt.cephuser;ceph-deploy osd prepare --zap <replaceable>node:vdb</replaceable></screen>
      <para>
       Be aware that 'zapping' erases the entire disk content.
      </para>
     </tip>
     <note>
<!-- bnc#912479 -->
      <title>Default File System for OSDs</title>
      <para>
       The default and only supported file system for OSDs is
       <literal>xfs</literal>.
      </para>
     </note>
     <para>
      Optionally, activate the OSD:
     </para>
<screen>&prompt.cephuser;ceph-deploy osd activate <replaceable>node:vdb1</replaceable></screen>
     <tip>
      <para>
       To join the functionality of <command>ceph-deploy osd
       prepare</command> and <command>ceph-deploy osd activate</command>,
       use <command>ceph-deploy osd create</command>.
      </para>
     </tip>
    </step>
    <step>
     <para>
      To test the status of the cluster, run
     </para>
<screen>sudo ceph -k ceph.client.admin.keyring health</screen>
    </step>
   </procedure>
   <tip>
    <title>Non-default Cluster Name</title>
    <para>
     If you need to install the cluster with <command>ceph-deploy</command>
     using a name other than the default <literal>cluster</literal> name,
     you need to initially specify it with <option>--cluster</option>, and
     then specify it in each <command>ceph-deploy</command> command related
     to that cluster:
    </para>
<screen>ceph-deploy --cluster my_cluster new [...]
ceph-deploy --ceph-conf my_cluster.conf mon create-initial
ceph-deploy --ceph-conf my_cluster.conf osd prepare [...]
ceph-deploy --ceph-conf my_cluster.conf osd activate [...]</screen>
    <para>
     Note that using a name other than default cluster name is not supported
     by SUSE.
    </para>
   </tip>
  </sect2>
 </sect1>
 <sect1 xml:id="ceph.install.crowbar">
  <title>Deploying with &crow;</title>

  <para>
   &crow; (<link xlink:href="http://crowbar.github.io/"/>) is a framework to
   build complete deployments. It helps you transform groups of bare-metal
   nodes into an operational cluster within relatively short time.
  </para>

  <tip>
   <title>More Details in the SUSE OpenStack Cloud Documentation</title>
   <para>
    As &crow; is used when deploying SUSE OpenStack Cloud nodes as well, you
    can find more detailed information in the related Deployment Guide at
    <link xlink:href="https://www.suse.com/documentation"/>.
   </para>
   <para>
    The following procedures outline the general workflow, emphasizing the
    points that are unique to &storage; 3.
   </para>
  </tip>

  <para>
   The deployment process consists of two basic steps: first you need to
   install and set up the &crow; admin server, then use it to deploy the
   available OSD/monitor nodes.
  </para>

  <sect2 xml:id="ceph.install.crowbar.admin_server">
   <title>Installing and Setting Up the &crow; Admin Server</title>
   <para>
    &crow; admin server is a stand-alone host with &sls; 12 SP1 installed,
    operating in the same network as the &ceph; OSD/MON nodes to be
    deployed. You need to configure the &crow; admin server so that it
    provides software repositories required for &ceph; deployment via TFTP
    protocol and PXE network boot.
   </para>
   <procedure>
    <step>
     <para>
      Install and register &sls; 12 SP1 on the &crow; admin server.
      Optionally, you can install and register the &storage; 3 extension at
      the same time. For more information on &sls; installation, see
      <link xlink:href="https://www.suse.com/documentation/sles-12/book_sle_deployment/data/cha_inst.html"/>.
      For more information on the extensions installation, see
      <link xlink:href="https://www.suse.com/documentation/sles-12/book_sle_deployment/data/cha_add-ons.html"/>.
     </para>
     <tip>
      <para>
       &crow; admin server does not require any graphical interface. To save
       the system resources and disk space, it is enough to install the
       <guimenu>Base System</guimenu>, <guimenu>Minimal System</guimenu>
       and, if you chose to install the &storage; 3 extension, <guimenu>SUSE
       Enterprise Storage Crowbar</guimenu> patterns from the
       <guimenu>Software Selection and System Tasks</guimenu> window. If you
       plan to synchronize repositories (see
       <xref linkend="ceph.install.crowbar.admin_server.repos"/>) with
       &smt;, add the <guimenu>Subscription Management Tool</guimenu>
       pattern as well.
      </para>
     </tip>
    </step>
    <step>
     <para>
      Configure network settings for the &crow; admin server. The server has
      to have a static IP address assigned, and the full host name including
      the domain name specified (for example
      <literal>crowbar-admin.example.com</literal>). Check with
      <command>hostname -f</command> if the host name resolves correctly.
      The local network where you deploy the cluster needs to have the DHCP
      server disabled as the &crow; admin server runs its own.
     </para>
     <tip>
      <para>
       &crow; admin server default IP address is 192.168.124.10. If it is
       possible to keep that IP in your network environment, you can save
       some time on reconfiguring the &crow; network settings.
      </para>
     </tip>
    </step>
    <step>
     <para>
      Configure NTP to keep the server's time synchronized. See
      <link xlink:href="https://www.suse.com/documentation/sles-12/book_sle_admin/data/cha_netz_xntp.html"/>
      for more information on the NTP protocol.
     </para>
    </step>
    <step>
     <para>
      Make sure that SSH is enabled and started on the server.
     </para>
    </step>
    <step>
     <para>
      Install and register the &storage; 3 extension if you did not install
      it in step 1. For more information on extension installation, see
      <link xlink:href="https://www.suse.com/documentation/sles-12/book_sle_deployment/data/cha_add-ons.html"/>.
      Then install the <guimenu>SUSE Enterprise Storage Crowbar</guimenu>
      pattern in &yast;. If you prefer the command line, run <command>sudo
      zypper in -t pattern ses_admin</command>.
     </para>
    </step>
    <step>
     <para>
      Mount software repositories required for &ceph; nodes deployment with
      &crow;. See <xref linkend="ceph.install.crowbar.admin_server.repos"/>
      for more detailed information.
     </para>
    </step>
    <step>
     <para>
      If you need to further customize the &crow; admin server settings,
      refer to the <emphasis>Crowbar Setup</emphasis> chapter of the current
      <emphasis>&ocloud; Deployment Guide</emphasis> at
      <link xlink:href="https://www.suse.com/documentation"/>.
     </para>
    </step>
    <step>
     <para>
      Run the <command>install-ses-admin</command> script to complete the
      &crow; admin server setup. The script outputs a lot of information to
      the <filename>/var/log/crowbar/install.log</filename> log file which
      can be examined in the case of failure. Run it in the
      <systemitem>screen</systemitem> environment for safety reasons, as the
      network will be reconfigured during its run and interrupts may occur.
     </para>
<screen>screen install-ses-admin</screen>
     <para>
      Be patient as the script takes several minutes to finish.
     </para>
    </step>
    <step>
     <para>
      After the script successfully finishes, you can view the &crow; admin
      server Web UI by pointing your Web browser to the &crow; admin server
      IP address (http://192.168.124.10 by default).
     </para>
    </step>
   </procedure>
   <sect3 xml:id="ceph.install.crowbar.admin_server.repos">
    <title>Prepare Software Repositories</title>
    <para>
     &crow; admin server needs to provide several software repositories so
     that the &ceph; nodes can install required packages from them on PXE
     boot. These repositories need to be mounted/synchronized under
     <filename>/srv/tftpboot/suse-12.1</filename>. The following description
     is based on the x86_64 architecture.
    </para>
    <tip>
     <title>Synchronizing Repositories</title>
     <para>
      There are several ways to provide the content in the repository
      directories. You can, for example, run your local &smt; instance,
      synchronize the repositories, and then export them via NFS and mount
      them on the &crow; admin server.
     </para>
    </tip>
    <variablelist>
     <varlistentry>
      <term>/srv/tftpboot/suse-12.1/x86_64/install</term>
      <listitem>
       <para>
        This directory needs to contain the contents of the SLES 12 SP1 DVD
        disc #1. &ceph; nodes need it for the base SLES 12 SP1 installation.
        You can either mount the downloaded .iso image as a loop device, or
        copy its content with <command>rsync</command>.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>/srv/tftpboot/suse-12.1/x86_64/repos/SLES12-SP1-Pool</term>
      <listitem>
       <para>
        Base software repository for SLES 12 SP1.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>/srv/tftpboot/suse-12.1/x86_64/repos/SLES12-SP1-Updates</term>
      <listitem>
       <para>
        Repository containing updates for SLES 12 SP1.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>/srv/tftpboot/suse-12.1/x86_64/repos/SUSE-Enterprise-Storage-3-Pool</term>
      <listitem>
       <para>
        Base software repository for SES 3.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>/srv/tftpboot/suse-12.1/x86_64/repos/SUSE-Enterprise-Storage-3-Updates</term>
      <listitem>
       <para>
        Repository containing updates for SES 3.
       </para>
      </listitem>
     </varlistentry>
    </variablelist>
    <para>
     For more detailed information on the repositories setup, refer to the
     <emphasis>Software Repository Setup</emphasis> chapter of the current
     <emphasis>&ocloud; Deployment Guide</emphasis> at
     <link xlink:href="https://www.suse.com/documentation"/>.
    </para>
   </sect3>
  </sect2>

  <sect2 xml:id="ceph.install.crowbar.deploy_nodes">
   <title>Deploying the &ceph; Nodes</title>
   <para>
    For a general outline of deploying OSD and monitor nodes, refer to the
    <emphasis>Installing the OpenStack Nodes</emphasis> chapter of the
    current <emphasis>&ocloud; Deployment Guide</emphasis> at
    <link xlink:href="https://www.suse.com/documentation"/>.
   </para>
   <note>
    <para>
     In the &crow; Web UI, contrary to &ocloud;, the &ceph; barclamp is
     accessible via <menuchoice><guimenu>Barclamps</guimenu><guimenu>SUSE
     Enterprise Storage</guimenu></menuchoice>. Use it to deploy &ceph;.
    </para>
   </note>
  </sect2>
 </sect1>
 <sect1 xml:id="ceph.install.saltstack">
  <title>Deploying with &salt;</title>

  <warning>
   <title>Technology Preview</title>
   <para>
    As of &storage; 3, &salt; is considered a technology preview and is not
    supported.
   </para>
  </warning>

  <para>
   &salt; is a <emphasis>stack</emphasis> of components that help you deploy
   and manage server infrastructure. It is very scalable, fast, and
   relatively easy to get running. Read the following considerations before
   you start deploying the cluster with &salt;:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     <emphasis>&smaster;</emphasis> is the host that controls the whole
     cluster deployment. Dedicate all the host resources to the &smaster;
     services. Do not install &ceph; on the host where you want to run
     &smaster;.
    </para>
   </listitem>
   <listitem>
    <para>
     <emphasis>&sminion;s</emphasis> are the nodes controlled by &smaster;.
     In the &ceph; environment, &sminion; is typically an OSD or monitor.
    </para>
   </listitem>
   <listitem>
    <para>
     &sminion;s need to correctly resolve the &smaster;'s host name over the
     network. By default, they look for the <systemitem>salt</systemitem>
     host name. Therefore, we recommend to set the &smaster;'s host name to
     <systemitem>salt</systemitem>.
    </para>
   </listitem>
  </itemizedlist>

  <para>
   To deploy &ceph; cluster using &salt;, follow these steps:
  </para>

  <procedure>
   <step>
    <para>
     Install and register &sls; 12 SP1 together with &storage; 3 extension
     on each node of the cluster.
    </para>
   </step>
   <step>
    <para>
     Install &smaster;:
    </para>
<screen>&prompt.smaster;zypper in salt-master</screen>
   </step>
   <step>
    <para>
     Install &sminion;s on each node you want to include in the &ceph;
     cluster:
    </para>
<screen>&prompt.sminion;zypper in salt-minion</screen>
   </step>
   <step>
    <para>
     Configure all &sminion;s to connect to the &smaster;. If your &smaster;
     is not reachable by the DNS name <literal>salt</literal>, you will need
     to edit the <filename>/etc/salt/minion</filename> file on each
     &sminion; and set
    </para>
<screen>master:<replaceable>dns_name_of_salt_master</replaceable></screen>
   </step>
   <step>
    <para>
     Set up salt keys.
    </para>
    <substeps>
     <step>
      <para>
       Start all &sminion;s:
      </para>
<screen>&prompt.sminion;systemctl start salt-minion</screen>
     </step>
     <step>
      <para>
       Start &smaster;:
      </para>
<screen>&prompt.smaster;systemctl start salt-master</screen>
     </step>
     <step>
      <para>
       Accept the keys from the minions:
      </para>
<screen>&prompt.smaster;salt-key --accept-all</screen>
      <note>
       <para>
        Both all &sminion;s and &smaster; have a key to do a symmetric
        key-based communication. The master must accept the minions into its
        control while minions with other keys are not accepted.
       </para>
      </note>
     </step>
    </substeps>
   </step>
   <step>
    <para>
     Install the <systemitem class="resource">salt-ceph</systemitem>
     package:
    </para>
<screen>&prompt.smaster;zypper in salt-ceph</screen>
   </step>
   <step>
    <para>
     Distribute the <systemitem>ceph</systemitem> execution interface to the
     minions:
    </para>
<screen>&prompt.smaster;salt "*" saltutil.sync_all</screen>
    <tip>
     <para>
      Use "*" in the <command>salt</command> command to specify all
      controlled minions.
     </para>
    </tip>
    <para>
     The <filename>ceph</filename> directory will be synchronized to each of
     your nodes. You can test the result with:
    </para>
<screen>&prompt.smaster;salt "*" ceph -d</screen>
    <para>
     If this returns no results, check and repeat the proceeding step.
    </para>
   </step>
   <step>
    <para>
     Create a directory on &smaster;:
    </para>
<screen>&prompt.smaster;mkdir /srv/salt/osceph/</screen>
   </step>
   <step>
    <para>
     Copy your desired <filename>ceph.conf</filename> into the new directory
     <filename>/srv/salt/osceph/</filename>. Note that the order of monitor
     host names and IP addresses is critical.
    </para>
   </step>
   <step>
    <para>
     Install &ceph; on the client nodes:
    </para>
<screen>&prompt.smaster;salt '*' pkg.install ceph</screen>
   </step>
   <step>
    <para>
     Optionally, install &rgw;s on the specified nodes:
    </para>
<screen>&prompt.smaster;salt 'rgw*' pkg.install ceph-radosgw</screen>
   </step>
   <step>
    <para>
     Create keys for each type of service used in the cluster&mdash;such as
     OSD, monitors, or MDS&mdash;and use them in the SLS file mentioned
     below:
    </para>
<screen>&prompt.smaster;salt '*' ceph.keyring_create type=<replaceable>keyring_type</replaceable></screen>
    <para>
     For more information on keys, keyring, and user management, see
     <xref linkend="cha.storage.cephx"/>.
    </para>
   </step>
   <step>
    <tip>
     <para>
      Salt uses <emphasis>Salt State</emphasis> files (or SLS files) that
      represent the state in which the cluster should be. SLS files use
      specific simple syntax. You can either use one complex file, or split
      it into several logical parts and include them from the main file.
     </para>
    </tip>
    <para>
     Get the example SLS file, modify it to your needs, and copy it into the
     <filename>/srv/salt/</filename> directory:
    </para>
<screen>cp /usr/share/doc/packages/salt-ceph/examples/cluster_buildup.sls /srv/salt</screen>
    <tip>
     <para>
      You can find the example SLS file with comments in
      <xref linkend="app.storage.sls"/>.
     </para>
    </tip>
   </step>
   <step>
    <para>
     Configure &ceph; following the SLS file you modified:
    </para>
    <tip>
     <para>
      The <command>state.sls</command> directive instructs Salt to apply the
      language included in the specified file.
     </para>
    </tip>
<screen>&prompt.smaster;cd /srv/salt
&prompt.smaster;salt '*' state.sls cluster_buildup</screen>
    <tip>
     <para>
      If you experience problems during this step, it is probably because of
      the settings, such as number of disks. To adjust these, try editing
      your <filename>cluster_buildup.sls</filename> file.
     </para>
    </tip>
   </step>
  </procedure>

  <para>
   You should now have a running &ceph; setup.
  </para>

  <tip>
   <title>Uninstalling &ceph;</title>
   <para>
    To uninstall the &ceph; cluster, copy
    <filename>cluster_teardown.sls</filename> to
    <filename>/srv/salt</filename>
   </para>
<screen>cp /usr/share/doc/packages/salt-ceph/examples/cluster_teardown.sls /srv/salt</screen>
   <para>
    change to <filename>/srv/salt</filename> and run
   </para>
<screen>&prompt.smaster;salt '*' state.sls cluster_teardown</screen>
  </tip>
 </sect1>
 <sect1 xml:id="ceph.install.calamari">
  <title>Install Calamari</title>

  <para>
   Calamari is a management and monitoring system for &ceph; storage
   cluster. It provides a Web user interface that makes &ceph; cluster
   monitoring very simple and handy.
  </para>

  <para>
   To install Calamari, run the following commands as &rootuser;:
  </para>

  <procedure>
   <step>
    <para>
     Install the client part of Calamari:
    </para>
<screen># zypper in romana</screen>
   </step>
   <step>
    <para>
     Initialize Calamari installation. You will be asked for superuser user
     name and password. These will be needed when logging in to the Web
     interface after the setup is complete.
    </para>
<screen># calamari-ctl initialize
[INFO] Loading configuration..
[INFO] Starting/enabling salt...
[INFO] Starting/enabling postgres...
[INFO] Initializing database...
[INFO] Initializing web interface...
[INFO] You will now be prompted for login details for the administrative user
account.  This is the account you will use to log into the web interface once
setup is complete.
Username (leave blank to use 'root'):  
Email address: 
Password: 
Password (again): 
Superuser created successfully.
[INFO] Starting/enabling services...
[INFO] Restarting services...
[INFO] Complete.</screen>
   </step>
   <step>
    <para>
     Check the firewall status
    </para>
<screen>sudo /sbin/SuSEfirewall2 status</screen>
    <para>
     and if it is off, check its configuration and turn it on with
    </para>
<screen>sudo /sbin/SuSEfirewall2 on</screen>
    <para>
     You can find detailed information in
     <xref linkend="storage.bp.net.firewall"/>.
    </para>
   </step>
   <step>
    <tip>
     <para>
      In order for Calamari to work correctly, the admin keyring needs to be
      installed on each monitor node:
     </para>
<screen>ceph-deploy admin mon1 mon2 mon3</screen>
     <para>
      where <replaceable>mon1</replaceable>,
      <replaceable>mon2</replaceable>, or <replaceable>mon3</replaceable>
      are the host names of the monitors.
     </para>
    </tip>
    <para>
     Now open your Web browser and point it to the host name/IP address of
     the server where you installed Calamari. Log in with the credentials
     you entered when installing the Calamari client. A welcome screen
     appears, instructing you to enter the <command>ceph-deploy calamari
     connect</command> command. Switch to the terminal on the Calamari host
     and enter the following command. Note that the
     <option>--master</option> option specifies the host name of the
     Calamari server to which all the cluster nodes connect to:
    </para>
<screen>ceph-deploy calamari connect --master <replaceable>master_host</replaceable> <replaceable>node1</replaceable> <replaceable>node2</replaceable> <replaceable>...</replaceable></screen>
    <para>
     After the command is successfully finished, reload the Web browser. Now
     you can monitor your &ceph; cluster, OSDs, pools, etc.
    </para>
    <important>
     <para>
      The Calamari dashboard screen shows the current status of the cluster.
      This updates regularly, so any change to the cluster state&mdash;for
      example if a node goes offline&mdash;should be reflected in Calamari
      within a few seconds. The <guimenu>Health</guimenu> panel includes a
      timer to indicate how long it has been since Calamari last saw
      heartbeat information from the cluster. Normally, this will not be
      more than one minute old, but in certain failure cases, for instance
      when a network outage occurs or if the cluster loses quorum (that is
      if more than half of the monitor nodes are down), Calamari will no
      longer be able to determine cluster state. In this case, the
      <guimenu>Health</guimenu> panel will indicate that the last update was
      more than one minute ago. After too long time with no updates,
      Calamari displays a warning at the top of the screen "Cluster Updates
      Are Stale. The Cluster is not updating Calamari." If this occurs, the
      other status information Calamari presents will not be correct so you
      should investigate further to check the status of your storage nodes
      and network.
     </para>
    </important>
    <tip>
     <para>
      They may be leftovers of the previous Calamari setup on the system. If
      after logging in to the Calamari application some nodes are already
      joined or registered, run the following on the Calamari host to
      trigger a re-run of salt on all &ceph; nodes, which should clear up
      any odd state or missing bits and pieces.
     </para>
<screen>salt '*' state.highstate</screen>
     <para>
      We also recommend to remove files from the previous Calamari setup,
      such as state files, configuration files, or PostgreSQL database
      files. At minimum, remove the files in the following directories:
     </para>
     <itemizedlist mark="bullet" spacing="normal">
      <listitem>
       <para>
        <filename>/etc/calamari/</filename>
       </para>
      </listitem>
      <listitem>
       <para>
        <filename>/etc/salt/</filename>
       </para>
      </listitem>
      <listitem>
       <para>
        <filename>/etc/graphite/</filename>
       </para>
      </listitem>
      <listitem>
       <para>
        <filename>/var/*/salt/</filename>
       </para>
      </listitem>
      <listitem>
       <para>
        <filename>/var/lib/graphite/</filename>
       </para>
      </listitem>
      <listitem>
       <para>
        <filename>/var/lib/pgsql/</filename>
       </para>
      </listitem>
     </itemizedlist>
    </tip>
   </step>
  </procedure>
 </sect1>
</chapter>
