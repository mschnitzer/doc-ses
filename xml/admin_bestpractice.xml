<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<!-- Converted by suse-upgrade version 1.1 -->
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="cha.storage.bestpractice">
 <title>Best Practice</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:maintainer>tbazant@suse.com</dm:maintainer>
   <dm:status>editing</dm:status>
   <dm:deadline></dm:deadline>
   <dm:priority></dm:priority>
   <dm:translation></dm:translation>
   <dm:languages></dm:languages>
   <dm:release>SES2.1</dm:release>
  </dm:docmanager>
 </info>
 <para>
  This chapter introduces a list of selected topics which you may encounter
  when managing the &ceph; environment. To every topic there is a
  recommended solution that helps you understand or fix the existing
  problem. The topics are sorted into relevant categories.
 </para>
 <sect1 xml:id="storage.bp.report_bug">
  <title>Reporting Software Problems</title>

  <para>
   If you come across a problem when running &storage; related to some of
   its components, such as &ceph;, &rgw;, or Calamari, report the problem to
   SUSE Technical Support. The recommended way is with the
   <command>supportconfig</command> utility.
  </para>

  <tip>
   <para>
    Because <command>supportconfig</command> is modular software, make sure
    that the <systemitem>supportutils-plugin-ses</systemitem> package is
    installed.
   </para>
<screen>rpm -q supportutils-plugin-ses</screen>
   <para>
    If it is missing on the &ceph; server, install it with
   </para>
<screen>zypper ref &amp;&amp; zypper in supportutils-plugin-ses</screen>
  </tip>

  <para>
   Although you can use <command>supportconfig</command> on the command
   line, we recommend using the related &yast; module. Find more information
   about <command>supportconfig</command> in
   <link xlink:href="https://www.suse.com/documentation/sles-12/singlehtml/book_sle_admin/book_sle_admin.html#sec.admsupport.supportconfig"/>.
  </para>
 </sect1>
 <sect1 xml:id="storage.bp.hwreq">
  <title>Hardware Recommendations</title>

  <para/>

  <sect2 xml:id="storage.bp.hwreq.replicas">
   <title>Can I Reduce Data Replication</title>
   <para>
    &ceph; stores data within pools. Pools are logical groups for storing
    objects. Data objects within a pool are replicated so that they can be
    recovered when OSDs fail. New pools are created with the default of
    three replicas. This number includes the 'original' data object itself.
    Three replicas then mean the data object and two its copies for a total
    of three instances.
   </para>
   <para>
    You can manually change the number of pool replicas (see
    <xref linkend="ceph.pools.options.num_of_replicas"/>). Setting a pool to
    two replicas means that there is only <emphasis>one</emphasis> copy of
    the data object besides the object itself, so if you lose one object
    instance, you have to trust that the other copy has not been corrupted
    for example since the last
    <link xlink:href="http://ceph.com/docs/master/rados/configuration/osd-config-ref/#scrubbing">scrubbing</link>
    during recovery.
   </para>
   <para>
    Setting a pool to one replica means that there is exactly
    <emphasis>one</emphasis> instance of the data object in the pool. If the
    OSD fails, you lose the data. A possible usage for a pool with one
    replica is storing temporary data for a short time.
   </para>
   <para>
    Setting more than three replicas for a pool means only a small increase
    in reliability, but may be suitable in rare cases. Remember that the
    more replicas, tho more disk space needed for storing the object copies.
    If you need the ultimate data security, we recommend using erasure coded
    pools. For more information, see <xref linkend="cha.ceph.erasure"/>.
   </para>
   <warning>
    <para>
     We strongly encourage you to either leave the number of replicas for a
     pool at the default value of 3, or use higher value if suitable.
     Setting the number of replicas to a smaller number is dangerous and may
     cause the loss of data stored in the cluster.
    </para>
   </warning>
  </sect2>

  <sect2 xml:id="storage.bp.hwreq.ec">
   <title>Can I Reduce Redundancy Similar to RAID 6 Arrays?</title>
   <para>
    When creating a new pool, &ceph; uses the replica type by default, which
    replicates objects across multiple disks to be able to recover from an
    OSD failure. While this type of pool is safe, it uses a lot of disk
    space to store objects.
   </para>
   <para>
    To reduce the disk space needed, &ceph; implements <emphasis>erasure
    coded</emphasis> pools. This method adds extra chunks of data to detect
    errors in a data stream. Erasure coded pools exhibit similar
    performance, reliability, and storage saved as RAID 6 arrays.
   </para>
   <para>
    As erasure coding is a complex topic, you need to study it properly to
    be able to deploy it for optimum performance. For more information, see
    <xref linkend="cha.ceph.erasure"/>.
   </para>
  </sect2>

  <sect2 xml:id="ses.bp.mindisk">
   <title>What is the Minimum Disk Size for an OSD node?</title>
   <para>
    There are two types of disk space needed to run on OSD: the space for
    the disk journal, and the space for the stored data. The minimum (and
    default) value for the journal is 6GB. The minimum space for data is 5GB
    as partitions smaller than 5GB are automatically assigned the weight of
    0.
   </para>
   <para>
    So although the minimum disk space for an OSD is 11GB, we do not
    recommend a disk smaller than 20GB, even for testing purposes.
   </para>
  </sect2>

  <sect2 xml:id="ses.bp.ram">
   <title>How Much RAM Do I Need in a Storage Server?</title>
   <para>
    The recommended minimum is 2GB per OSD. Note that during recovery, 1 or
    even 2GB of RAM per terabyte of OSD disk space is optimal.
   </para>
  </sect2>

  <sect2 xml:id="ses.bp.diskshare">
   <title>OSD and Monitor Sharing One Server</title>
   <para>
    Although it is technically possible to run OSDs and monitor nodes on the
    same server in test environments, we strongly recommend having a
    separate server for each monitor node in production. The main reason is
    performance&mdash;the more OSDs the cluster has, the more I/O operations
    the monitor nodes need to perform. And when one server is shared between
    a monitor node and OSD(s), the OSD I/O operations are a limiting factor
    for the monitor node.
   </para>
   <para>
    Another aspect is whether to share disks between an OSD, a monitor node,
    and the operating system on the server. The answer is simple: if
    possible, dedicate a separate disk to OSD, and a separate server to a
    monitor node.
   </para>
   <para>
    Although &ceph; supports directory-based OSDs, an OSD should always have
    a dedicated disk other than the operating system one.
   </para>
   <tip>
    <para>
     If it is <emphasis>really</emphasis> necessary to run OSD and monitor
     node on the same server, run the monitor on a separate disk by mounting
     the disk to the <filename>/var/lib/ceph/mon</filename> directory for
     slightly better performance.
    </para>
   </tip>
  </sect2>

  <sect2 xml:id="ses.bp.numofdisks">
   <title>How Many Disks Can I Have in a Server</title>
   <para>
    You can have as many disks in one server as it allows. There are a few
    things to consider when planning the number of disks per server:
   </para>
   <itemizedlist mark="bullet" spacing="normal">
    <listitem>
     <para>
      <emphasis>Network bandwidth.</emphasis> The more disks you have in a
      server, the more data must be transferred via the network card(s) for
      the disk write operations.
     </para>
    </listitem>
    <listitem>
     <para>
      <emphasis>Memory.</emphasis> For optimum performance, reserve at least
      2GB of RAM per terabyte of disk space installed.
     </para>
    </listitem>
    <listitem>
     <para>
      <emphasis>Fault tolerance.</emphasis> If the complete server fails,
      the more disks it has, the more OSDs the cluster temporarily loses.
      Moreover, to keep the replication rules running, you need to copy all
      the data from the failed server between the other nodes in the
      cluster.
     </para>
    </listitem>
   </itemizedlist>
  </sect2>

  <sect2 xml:id="ses.bp.share_ssd_journal">
   <title>How Many OSDs Can Share a Single SSD Journal</title>
   <para>
    Solid-state drives (SSD) have no moving parts. This reduces random
    access time and read latency while accelerating data throughput. Because
    their price per 1MB is significantly higher than the price of spinning
    hard disks, SSDs are only suitable for smaller storage.
   </para>
   <para>
    OSDs may see a significant performance improvement by storing their
    journal on an SSD and the object data on a separate hard disk. The
    <option>osd journal</option> configuration setting defaults to
    <filename>/var/lib/ceph/osd/<replaceable>cluster</replaceable>-<replaceable>id</replaceable>/journal</filename>.
    You can mount this path to an SSD or to an SSD partition so that it is
    not merely a file on the same disk as the object data.
   </para>
   <tip>
    <title>Sharing an SSD for Multiple Journals</title>
    <para>
     As journal data occupies relatively small space, you can mount several
     journal directories to a single SSD disk. Keep in mind that with each
     shared journal, the performance of the SSD disk degrades. We do not
     recommend sharing more than 4 journals on the same SSD disk.
    </para>
   </tip>
  </sect2>
 </sect1>
 <sect1 xml:id="storage.bp.inst">
  <title>Installation</title>

  <para/>

  <sect2 xml:id="storage.bp.inst.cephdeploy_usage">
   <title>Using <command>ceph-deploy</command> on an Already Setup Server</title>
   <para>
    <command>ceph-deploy</command> is a command line utility to easily
    deploy a &ceph; cluster (see
    <xref linkend="ceph.install.ceph-deploy"/>). After the cluster is
    deployed, you can use <command>ceph-deploy</command> to administer the
    clusters' nodes. You can add OSD nodes, monitor nodes, gather
    authentication keys, or purge a running cluster.
    <command>ceph-deploy</command> has the following general syntax:
   </para>
<screen>ceph-deploy <replaceable>subcommands</replaceable> <replaceable>options</replaceable></screen>
   <para>
    A list of selected <command>ceph-deploy</command> subcommands with short
    descriptions follow.
   </para>
   <tip>
    <para>
     Administer &ceph; nodes with <command>ceph-deploy</command> from the
     admin node. Before administering them, always create a new temporary
     directory and <command>cd</command> into it. Then choose one monitor
     node and gather the authentication keys with the
     <command>gatherkeys</command> subcommand from it, and copy the
     <filename>/etc/ceph/ceph.conf</filename> file from the monitor node
     into the current local directory.
    </para>
<screen>&prompt.cephuser; mkdir ceph_tmp
&prompt.cephuser; cd ceph_tmp
&prompt.cephuser; ceph-deploy gatherkeys ceph_mon_host
&prompt.cephuser; scp ceph_mon_host:/etc/ceph/ceph.conf .</screen>
   </tip>
   <variablelist>
    <varlistentry>
     <term>gatherkeys</term>
     <listitem>
      <para>
       Gather authentication keys for provisioning new nodes. It takes host
       names as arguments. It checks for and fetches <literal>client.admin
       keyring</literal>, monitor keyring and
       <literal>bootstrap-mds/bootstrap-osd</literal> keyring from monitor
       host. These authentication keys are used when new
       <literal>monitors/OSDs/MDS</literal> are added to the cluster.
      </para>
      <para>
       Usage:
      </para>
<screen>ceph-deploy gatherkeys <replaceable>hostname</replaceable></screen>
      <para>
       <replaceable>hostname</replaceable> is the host name of the monitor
       from where keys are to be pulled.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>mon add</term>
     <listitem>
      <para>
       Adds a monitor to an existing cluster. It first detects the platform
       and distribution for the target host, and checks if the host name is
       compatible for deployment. It then uses the monitor keyring, ensures
       configuration for new monitor host and adds the monitor to the
       cluster. If the section for the monitor exists, it can define the
       monitor address by the <literal>mon addr</literal> option, otherwise
       it will fall back by resolving the host name to an IP. If
       <option>--address</option> is used, it will override all other
       options. After adding the monitor to the cluster, it gives it some
       time to start. It then looks for any monitor errors, and checks
       monitor status. Monitor errors arise if the monitor is not added in
       the <option>mon initial members</option> option, if it does not exist
       in <option>monmap</option>, or if neither
       <option>public_addr</option> nor <option>public_network</option> keys
       were defined for monitors. Under such conditions, monitors may not be
       able to form quorum. Monitor status tells if the monitor is up and
       running normally. The status is checked by running <command>ceph
       daemon mon.hostname mon_status</command> on remote end which provides
       the output and returns a Boolean status of what is going on.
       <literal>False</literal> means a monitor that is not fine even if it
       is up and running, while <literal>True</literal> means the monitor is
       up and running correctly.
      </para>
      <para>
       Usage:
      </para>
<screen>ceph-deploy mon add <replaceable>host</replaceable>
ceph-deploy mon add <replaceable>host</replaceable> <option>--address <replaceable>IP</replaceable></option></screen>
      <para>
       <replaceable>host</replaceable> is the host name and
       <replaceable>IP</replaceable> is the IP address of the desired
       monitor node.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>osd prepare</term>
     <listitem>
      <para>
       Prepares a directory, disk or drive for a &ceph; OSD. It first checks
       against multiple OSDs getting created and warns about the possibility
       of more than the recommended which would cause issues with max
       allowed PIDs in a system. It then reads the bootstrap-osd key for the
       cluster or writes the bootstrap key if not found. It then uses
       <command>ceph-disk</command> utility’s <command>prepare</command>
       subcommand to prepare the disk and journal and deploy the OSD on the
       desired host. It gives some time to the OSD to settle and checks for
       any possible errors and if found, reports them to the user.
      </para>
      <para>
       Usage:
      </para>
<screen>ceph-deploy osd prepare <replaceable>host</replaceable>:<replaceable>disk</replaceable>[<replaceable>journal</replaceable>] ...</screen>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>osd activate</term>
     <listitem>
      <para>
       Activates the OSD prepared using the <command>prepare</command>
       subcommand. It actually uses <command>ceph-disk</command> utility’s
       <command>activate</command> subcommand to activate the OSD with the
       appropriate initialization type based on the distribution. Once
       activated, it gives some time to the OSD to start and checks for any
       possible errors and if found, reports to the user. It checks the
       status of the prepared OSD, checks the OSD tree and makes sure the
       OSDs are up and in.
      </para>
      <tip>
       <para>
        <command>osd activate</command> is usually not needed as
        <systemitem>udev</systemitem> rules explicitly trigger "activate"
        after a disk is prepared after <command>osd prepare</command>.
       </para>
      </tip>
      <para>
       Usage:
      </para>
<screen>ceph-deploy osd activate <replaceable>host</replaceable>:<replaceable>disk</replaceable>[<replaceable>journal</replaceable>] ...</screen>
      <tip>
       <para>
        You can use <command>ceph-deploy osd create</command> to join
        <command>prepare</command> and <command>activate</command>
        functionality into one command.
       </para>
      </tip>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>rgw prepare/activate/create</term>
     <listitem>
      <para>
       Find more information in <xref linkend="storage.bp.inst.rgw"/>.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>purge, purgedata, forgetkeys</term>
     <listitem>
      <para>
       You can use the subcommands to completely purge the &ceph; cluster
       (or some of its nodes) as if &ceph; was never installed on the
       cluster servers. They are typically used when &ceph; installation
       fails and you want to start with a clean environment. Or, you can
       purge one or more nodes because you want to remove them from the
       cluster as their life-cycle ends.
      </para>
      <para>
       For more information on purging the cluster or its nodes, see
       <xref linkend="ceph.install.ceph-deploy.purge"/>.
      </para>
      <tip>
       <para>
        If you do not intend to purge the whole cluster, do not use the
        <command>forgetkeys</command> subcommand, as the keys will remain in
        place for the remaining cluster infrastructure.
       </para>
      </tip>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect2>

  <sect2 xml:id="storage.bp.inst.add_osd_cephdisk">
   <title>Adding OSDs with <command>ceph-disk</command></title>
   <para>
    <command>ceph-disk</command> is a utility that can prepare and activate
    a disk, partition or directory as a &ceph; OSD. It automates the
    multiple steps involved in manual creation and start of an OSD into two
    steps of preparing and activating the OSD by using the subcommands
    <command>prepare</command> and <command>activate</command>.
   </para>
   <variablelist>
    <varlistentry>
     <term><command>prepare</command>
     </term>
     <listitem>
      <para>
       Prepares a directory, disk or drive for a &ceph; OSD. It creates a
       GPT partition, marks the partition with &ceph; type uuid, creates a
       file system, marks the file system as ready for &ceph; consumption,
       uses entire partition and adds a new partition to the journal disk.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><command>activate</command>
     </term>
     <listitem>
      <para>
       Activates the &ceph; OSD. It mounts the volume in a temporary
       location, allocates an OSD ID (if needed), remounts in the correct
       location
       <filename>/var/lib/ceph/osd/<replaceable>cluster</replaceable>-<replaceable>id</replaceable></filename>
       and starts <command>ceph-osd</command>.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
   <para>
    The following example shows steps for adding an OSD with
    <command>ceph-osd</command>.
   </para>
   <procedure>
    <step>
     <para>
      Make sure a new disk is physically present on the node where you want
      to add the OSD. In our example, it is <emphasis>node1</emphasis>
      belonging to cluster <emphasis>ceph</emphasis>.
     </para>
    </step>
    <step>
     <para>
      <command>ssh</command> to node1.
     </para>
    </step>
    <step>
     <para>
      Generate a unique identification for the new OSD:
     </para>
<screen>uuidgen
c70c032a-6e88-4962-8376-4aa119cb52ee</screen>
    </step>
    <step>
     <para>
      Prepare the disk:
     </para>
<screen>sudo ceph-disk prepare --cluster ceph \
--cluster-uuid c70c032a-6e88-4962-8376-4aa119cb52ee --fs-type xfs /dev/hdd1</screen>
    </step>
    <step>
     <para>
      Activate the OSD:
     </para>
<screen>sudo ceph-disk activate /dev/hdd1</screen>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="storage.bp.inst.add_osd_cephdeploy">
   <title>Adding OSDs with <command>ceph-deploy</command></title>
   <para>
    <command>ceph-deploy</command> is a command line utility to simplify the
    installation and configuration of a &ceph; cluster. It can be used to
    add or remove OSDs as well. To add a new OSD to a node
    <literal>node2</literal> with <command>ceph-deploy</command>, follow
    these steps:
   </para>
   <tip>
    <para>
     <command>ceph-deploy</command> is usually run from the administration
     node, from which you installed the cluster.
    </para>
   </tip>
   <procedure>
    <step>
     <para>
      List available disks on a node:
     </para>
<screen>ceph-deploy disk list node2
[...]
[node2][DEBUG ] /dev/sr0 other, unknown
[node2][DEBUG ] /dev/vda :
[node2][DEBUG ]  /dev/vda1 swap, swap
[node2][DEBUG ]  /dev/vda2 other, btrfs, mounted on /
[node2][DEBUG ] /dev/vdb :
[node2][DEBUG ]  /dev/vdb1 ceph data, active, cluster ceph, osd.1, journal /dev/vdb2
[node2][DEBUG ]  /dev/vdb2 ceph journal, for /dev/vdb1
[node2][DEBUG ] /dev/vdc other, unknown</screen>
     <para>
      <filename>/dev/vdc</filename> seems to be unused, so let us focus on
      adding it as an OSD.
     </para>
    </step>
    <step>
     <para>
      Zap the disk. Zapping deletes the disk's partition table.
     </para>
<screen>ceph-deploy disk zap node2:vdc</screen>
     <warning>
      <para>
       Zapping deletes all data from the disk
      </para>
     </warning>
    </step>
    <step>
     <para>
      Prepare the OSD. The <command>prepare</command> command expects you to
      specify the disk for data, and optionally the disk for its journal. We
      recommend storing the journal on a separate drive to maximize
      throughput.
     </para>
<screen>ceph-deploy osd prepare node2:vdc:/dev/ssd</screen>
    </step>
<!-- 2015-11-05 tbazant: not needed, so commenting out
    <step>
     <para>
      Activate the OSD. The <command>activate</command> command will cause
      your OSD to come <literal>up</literal> and be placed
      <literal>in</literal> the cluster. The <command>activate</command>
      command uses the path to the partition created when running the
      <command>prepare</command> command.
     </para>
<screen>ceph-deploy osd activate node2:/dev/vdc1:/dev/ssd1</screen>
    </step>
    -->
   </procedure>
  </sect2>

  <sect2 xml:id="storage.bp.inst.add_rm_monitor">
   <title>Adding and Removing Monitors</title>
   <para>
    With <command>ceph-deploy</command>, adding and removing monitors is a
    simple task. Also, take into account the following
    restrictions/recommendation.
   </para>
   <important>
    <itemizedlist mark="bullet" spacing="normal">
     <listitem>
      <para>
       <command>ceph-deploy</command> restricts you to only install one
       monitor per host.
      </para>
     </listitem>
     <listitem>
      <para>
       We do not recommend mixing monitors and OSDs on the same host.
      </para>
     </listitem>
     <listitem>
      <para>
       For high availability, you should run a production &ceph; cluster
       with <emphasis>at least</emphasis> three monitors.
      </para>
     </listitem>
    </itemizedlist>
   </important>
   <sect3 xml:id="storage.bp.inst.add_rm_monitor.addmon">
    <title>Adding a Monitor</title>
    <para>
     After you create a cluster and install &ceph; packages to the monitor
     host(s) (see <xref linkend="ceph.install.ceph-deploy"/> for more
     information), you may deploy the monitors to the monitor hosts. You may
     specify more monitor host names in the same command.
    </para>
<screen>ceph-deploy mon create <replaceable>host-name</replaceable></screen>
    <note>
     <para>
      When adding a monitor on a host that was not in hosts initially
      defined with the <command>ceph-deploy new</command> command, a
      <option>public network</option> statement needs to be added to the
      <filename>ceph.conf</filename> file.
     </para>
    </note>
   </sect3>
   <sect3 xml:id="storage.bp.inst.add_rm_monitor.rmmon">
    <title>Removing a Monitor</title>
    <para>
     If you have a monitor in your cluster that you want to remove, you may
     use the destroy option. You may specify more monitor host names in the
     same command.
    </para>
<screen>ceph-deploy mon destroy <replaceable>host-name</replaceable></screen>
    <note>
     <para>
      Ensure that if you remove a monitor, the remaining monitors will be
      able to establish a consensus. If that is not possible, consider
      adding a monitor before removing the monitor you want to take offline.
     </para>
    </note>
   </sect3>
  </sect2>

  <sect2 xml:id="storage.bp.inst.rgw">
   <title>Usage of <command>ceph-deploy rgw</command></title>
   <para>
    The <command>ceph-deploy</command> script includes the
    <command>rgw</command> component that helps you manage &rgw; instances.
    Its general form follows this pattern:
   </para>
<screen>ceph-deploy rgw <replaceable>subcommand</replaceable> <replaceable>rgw-host</replaceable>:<replaceable>rgw-instance</replaceable>:<replaceable>fqdn</replaceable>:<replaceable>port</replaceable>:<replaceable>redirect</replaceable></screen>
   <variablelist>
    <varlistentry>
     <term>subcommand</term>
     <listitem>
      <para>
       One of <command>list</command>, <command>prepare</command>,
       <command>activate</command>, <command>create</command> (=
       <command>prepare</command> + <command>activate</command>), or
       <command>delete</command>.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>rgw-host</term>
     <listitem>
      <para>
       Host name where you want to operate the &rgw;.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>rgw-instance</term>
     <listitem>
      <para>
       &ceph; instance name. Default is 'rgw-host'.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>fqdn</term>
     <listitem>
      <para>
       Virtual host to listen to. Default is 'None'.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>port</term>
     <listitem>
      <para>
       Port to listen to. Default is 80.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>redirect</term>
     <listitem>
      <para>
       The URL redirect. Default is '^/(.*)'.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
   <para>
    For example:
   </para>
<screen>ceph-deploy rgw prepare example_host2:gateway1</screen>
   <para>
    or
   </para>
<screen>ceph-deploy activate example_host1:gateway1:virtual_srv2:81</screen>
   <tip>
    <title>Specifying Multiple &rgw; Instances</title>
    <para>
     You can specify more <option>rgw_hostname:rgw_instance</option> pairs
     on the same command line if you separate them with a comma:
    </para>
<screen>ceph-deploy rgw create hostname1:rgw,hostname2:rgw,hostname3:rgw</screen>
   </tip>
   <para>
    For a practical example of setting &rgw; with
    <command>ceph-deploy</command>, see <xref linkend="ses.rgw.config"/>.
   </para>
  </sect2>

  <sect2 xml:id="storage.bp.inst.rgw_client">
   <title>&rgw; Client Usage</title>
   <para>
    To use &rgw; REST interfaces, you need to create a user for the S3
    interface, then a subuser for the Swift interface. Find more information
    on creating &rgw; users in <xref linkend="storage.bp.account.swiftadd"/>
    and <xref linkend="storage.bp.account.s3add"/>.
   </para>
   <sect3>
    <title>S3 Interface Access</title>
    <para>
     To access the S3 interface, you need to write a Python script. The
     script will connect to &rgw;, create a new bucket, and list all
     buckets. The values for <option>aws_access_key_id</option> and
     <option>aws_secret_access_key</option> are taken from the values of
     <option>access_key</option> and <option>secret_key</option> returned by
     the <command>radosgw_admin</command> command from
     <xref linkend="storage.bp.account.s3add"/>.
    </para>
    <procedure>
     <step>
      <para>
       Install the <systemitem>python-boto</systemitem> package:
      </para>
<screen>sudo zypper in python-boto</screen>
     </step>
     <step>
      <para>
       Create a new Python script called <filename>s3test.py</filename> with
       the following content:
      </para>
<screen>import boto
import boto.s3.connection
access_key = '11BS02LGFB6AL6H1ADMW'
secret_key = 'vzCEkuryfn060dfee4fgQPqFrncKEIkh3ZcdOANY'
conn = boto.connect_s3(
aws_access_key_id = access_key,
aws_secret_access_key = secret_key,
host = '{hostname}',
is_secure=False,
calling_format = boto.s3.connection.OrdinaryCallingFormat(),
)
bucket = conn.create_bucket('my-new-bucket')
for bucket in conn.get_all_buckets():
print "{name}\t{created}".format(
name = bucket.name,
created = bucket.creation_date,
)</screen>
      <para>
       Replace <literal>{hostname}</literal> with the host name of the host
       where you configured &rgw; service, for example
       <literal>gateway_host</literal>.
      </para>
     </step>
     <step>
      <para>
       Run the script:
      </para>
<screen>python s3test.py</screen>
      <para>
       The script outputs something like the following:
      </para>
<screen>my-new-bucket 2015-07-22T15:37:42.000Z</screen>
     </step>
    </procedure>
   </sect3>
   <sect3>
    <title>Swift Interface Access</title>
    <para>
     To access &rgw; via Swift interface, you need the
     <command>swift</command> command line client. Its manual page
     <command>man 1 swift</command> tells you more about its command line
     options.
    </para>
    <para>
     To install <command>swift</command>, run the following:
    </para>
<screen>sudo zypper in python-swiftclient</screen>
    <para>
     The swift access uses the following syntax:
    </para>
<screen>swift -A http://<replaceable>IP_ADDRESS</replaceable>/auth/1.0 \
-U example_user:swift -K '<replaceable>swift_secret_key</replaceable>' list</screen>
    <para>
     Replace <replaceable>IP_ADDRESS</replaceable> with the IP address of
     the gateway server, and <replaceable>swift_secret_key</replaceable>
     with its value from the output of the <command>radosgw-admin key
     create</command> command executed for the
     <systemitem>swift</systemitem> user in
     <xref linkend="storage.bp.account.swiftadd"/>.
    </para>
    <para>
     For example:
    </para>
<screen>swift -A http://gateway.example.com/auth/1.0 -U example_user:swift \
-K 'r5wWIxjOCeEO7DixD1FjTLmNYIViaC6JVhi3013h' list</screen>
    <para>
     The output is:
    </para>
<screen>my-new-bucket</screen>
   </sect3>
  </sect2>
 </sect1>
 <sect1 xml:id="storage.bp.monitoring">
  <title>Monitoring</title>

  <para/>

  <sect2 xml:id="storage.bp.monitoring.calamari_usage_graphs">
   <title>Usage Graphs on Calamari</title>
   <para>
    Calamari&mdash;a &ceph;'s Web front-end for managing and monitoring the
    cluster&mdash;includes several graphs on the cluster's usage.
   </para>
   <para>
    At the bottom of the <guimenu>Dashboard</guimenu>&mdash;the home page of
    Calamari&mdash;there are two usage related boxes. While
    <guimenu>IOPS</guimenu> shows the cluster's overall number of
    input/output operations per second, the <guimenu>Usage</guimenu> graph
    shows the number of the cluster's total/used disk space.
   </para>
   <figure>
    <title>IOPS and Usage Graphs on Calamari Dashboard</title>
    <mediaobject>
     <imageobject role="fo">
      <imagedata fileref="calamari_iops_usage.png" width="80%" format="PNG"/>
     </imageobject>
     <imageobject role="html">
      <imagedata fileref="calamari_iops_usage.png" width="60%" format="PNG"/>
     </imageobject>
    </mediaobject>
   </figure>
   <para>
    You can find more detailed and interactive graphs after clicking the
    <guimenu>Charts</guimenu> menu item. It shows the cluster's overall
    input/output operations per second and free disk space by default.
    Select <guimenu>Pool IOPS</guimenu> from the top drop-down box to detail
    the view by existing pools.
   </para>
   <figure>
    <title>Pool IOPS Detailed View</title>
    <mediaobject>
     <imageobject role="fo">
      <imagedata fileref="calamari_mypool_iops.png" width="80%" format="PNG"/>
     </imageobject>
     <imageobject role="html">
      <imagedata fileref="calamari_mypool_iops.png" width="70%" format="PNG"/>
     </imageobject>
    </mediaobject>
   </figure>
   <para>
    By moving the slider in the <guimenu>Time Axis</guimenu> pane, you can
    change the displayed time interval in the graph. By moving the mouse
    over the graph, the time/read/write information changes accordingly. By
    clicking and dragging the mouse horizontally across the graph, the
    specified time interval gets zoomed. You can see more help by moving the
    mouse over the little question mark in the top right corner of the
    graph.
   </para>
   <para>
    If you select the host name of a specific &ceph; server, Calamari
    displays detailed information about CPU, average load, and memory
    related to the specified host.
   </para>
   <figure>
    <title>&ceph; Host Average Load</title>
    <mediaobject>
     <imageobject role="fo">
      <imagedata fileref="calamari_host_avgload.png" width="80%" format="PNG"/>
     </imageobject>
     <imageobject role="html">
      <imagedata fileref="calamari_host_avgload.png" width="70%" format="PNG"/>
     </imageobject>
    </mediaobject>
   </figure>
  </sect2>

  <sect2 xml:id="storage.bp.monitoring.fullosd">
   <title>Checking for Full OSDs</title>
   <para>
    &ceph; prevents you from writing to a full OSD so that you do not lose
    data. In an operational cluster, you should receive a warning when your
    cluster is getting near its full ratio. The <command>mon osd full
    ratio</command> defaults to 0.95, or 95% of capacity before it stops
    clients from writing data. The <command>mon osd nearfull ratio</command>
    defaults to 0.85, or 85% of capacity, when it generates a health
    warning.
   </para>
   <para>
    Full OSD nodes will be reported by <command>ceph health</command>:
   </para>
<screen>ceph health
  HEALTH_WARN 1 nearfull osds
  osd.2 is near full at 85%</screen>
   <para>
    or
   </para>
<screen>ceph health
  HEALTH_ERR 1 nearfull osds, 1 full osds
  osd.2 is near full at 85%
  osd.3 is full at 97%</screen>
   <para>
    The best way to deal with a full cluster is to add new OSD nodes
    allowing the cluster to redistribute data to the newly available
    storage.
   </para>
   <para>
    If you cannot start an OSD because it is full, you may delete some data
    by deleting some placement group directories in the full OSD.
   </para>
   <tip>
    <title>Preventing Full OSDs</title>
    <para>
     After an OSD becomes full&mdash;is uses 100% of its disk
     space&mdash;it will normally crash quickly without warning. Following
     are a few tips to remember when administering OSD nodes.
    </para>
    <itemizedlist mark="bullet" spacing="normal">
     <listitem>
      <para>
       Each OSD's disk space (usually mounted under
       <filename>/var/lib/ceph/osd/osd-{1,2..}</filename>) needs to be
       placed on a dedicated underlying disk or partition.
      </para>
     </listitem>
     <listitem>
      <para>
       Check the &ceph; configuration files and make sure that &ceph; does
       not store its log file to the disks/partitions dedicated for use by
       OSDs.
      </para>
     </listitem>
     <listitem>
      <para>
       Make sure that no other process writes to the disks/partitions
       dedicated for use by OSDs.
      </para>
     </listitem>
    </itemizedlist>
   </tip>
  </sect2>

  <sect2 xml:id="storage.bp.monitoring.osd">
   <title>Checking if OSD Daemons are Running on a Node</title>
   <para>
    To check the status of OSD services on a specific node, log in to the
    node, and run the following:
   </para>
<screen>sudo systemctl status ceph-osd*
 ceph-osd@0.service - Ceph object storage daemon
    Loaded: loaded (/usr/lib/systemd/system/ceph-osd@.service; enabled)
    Active: active (running) since Fri 2015-02-20 11:13:18 CET; 2 days ago
  Main PID: 1822 (ceph-osd)
    CGroup: /system.slice/system-ceph\x2dosd.slice/ceph-osd@0.service
            └─1822 /usr/bin/ceph-osd -f --cluster ceph --id 0</screen>
   <para>
    For more information, see <xref linkend="ceph.operating.services"/>.
   </para>
  </sect2>

  <sect2 xml:id="storage.bp.monitoring.mon">
   <title>Checking if Monitor Daemons are Running on a Node</title>
   <para>
    To check the status of monitor services on a specific node, log in to
    the node, and run the following:
   </para>
<screen>sudo systemctl status ceph-mon*
 ceph-mon@doc-ceph1.service - Ceph cluster monitor daemon
    Loaded: loaded (/usr/lib/systemd/system/ceph-mon@.service; enabled)
    Active: active (running) since Wed 2015-02-18 16:57:17 CET; 4 days ago
  Main PID: 1203 (ceph-mon)
    CGroup: /system.slice/system-ceph\x2dmon.slice/ceph-mon@doc-ceph1.service
            └─1203 /usr/bin/ceph-mon -f --cluster ceph --id doc-ceph1</screen>
   <para>
    For more information, see <xref linkend="ceph.operating.services"/>.
   </para>
  </sect2>

  <sect2 xml:id="storage.bp.monitoring.diskfails">
   <title>What Happens When a Disk Fails?</title>
   <para>
    When a disk with a stored cluster data has a hardware problem and fails
    to operate, here is what happens:
   </para>
   <itemizedlist mark="bullet" spacing="normal">
    <listitem>
     <para>
      The related OSD crashed and is automatically removed from the cluster.
     </para>
    </listitem>
    <listitem>
     <para>
      The failed disk's data is replicated to another OSD in the cluster
      from other copies of the same data stored in other OSDs.
     </para>
    </listitem>
    <listitem>
     <para>
      Then you should remove the disk from the cluster &crushmap;, and
      physically from the host hardware.
     </para>
    </listitem>
   </itemizedlist>
  </sect2>

  <sect2 xml:id="storage.bp.monitoring.journalfails">
   <title>What Happens When a Journal Disk Fails?</title>
   <para>
    &ceph; OSDs use journaling file systems (see
    <link xlink:href="http://en.wikipedia.org/wiki/Journaling_file_system"/>
    for more information) to store data. When a disk dedicated to a journal
    fails, the related OSD(s) fail as well (see
    <xref linkend="storage.bp.monitoring.diskfails"/>).
   </para>
   <warning>
    <title>Hosting Multiple Journals on One Disk</title>
    <para>
     For performance boost, you can use a fast disk (such as SSD) to store
     journal partitions for several OSDs. We do not recommend to host
     journals for more than 4 OSDs on one disk, because in case of the
     journals' disk failure, you risk losing stored data for all the related
     OSDs' disks.
    </para>
   </warning>
  </sect2>
 </sect1>
 <sect1 xml:id="storage.bp.disk">
  <title>Disk Management</title>

  <para/>

  <sect2 xml:id="storage.bp.disk.add">
   <title>Adding Disks</title>
   <important>
    <itemizedlist mark="bullet" spacing="normal">
     <listitem>
      <para>
       This can be done on a live cluster without downtime.
      </para>
     </listitem>
     <listitem>
      <para>
       This will cause increased replication traffic between servers.
      </para>
     </listitem>
     <listitem>
      <para>
       Doing this operation repeatedly before the last operation has
       completed replication can save the cluster overall rebuild time.
      </para>
     </listitem>
    </itemizedlist>
   </important>
   <para>
    To add a disk (<filename>/dev/sdd</filename> in our example) to a &ceph;
    cluster, follow these steps:
   </para>
   <procedure>
    <step>
     <para>
      Create a partition <literal>sdd1</literal> on the disk:
     </para>
<screen>sudo parted /dev/sdd1 mkpart primary 0.0 -1s</screen>
    </step>
    <step>
     <para>
      Format the partition with XFS file system:
     </para>
<screen>sudo mkfs.xfs -f /dev/sdd1</screen>
    </step>
    <step>
     <para>
      Find out the UUID (Universally Unique Identifier) of the disk:
     </para>
<screen>ls -l /dev/disk/by-uuid | grep sdd1
 [...] 04bb24f1-d631-47ff-a2ee-22d94ad4f80c -&gt; ../../sdd1</screen>
    </step>
    <step>
     <para>
      Add the corresponding line to <filename>/etc/fstab</filename> for the
      example disk <literal>osd.12</literal>:
     </para>
<screen>[...]
 UUID=04bb24f1-d631-47ff-a2ee-22d94ad4f80c /mnt/osd.12 xfs \
 defaults,errors=remount-ro 0 1
 [...]</screen>
    </step>
    <step>
     <para>
      Mount the disk:
     </para>
<screen>sudo mount /mnt/osd.12</screen>
    </step>
    <step>
     <para>
      Add the new disk to <filename>/etc/ceph/ceph.conf</filename> and copy
      the updated configuration file to all other nodes in the cluster.
     </para>
    </step>
    <step>
     <para>
      Create the OSD:
     </para>
<screen>ceph osd create 04bb24f1-d631-47ff-a2ee-22d94ad4f80c</screen>
    </step>
    <step>
     <para>
      Make sure that the new OSD is accepted into the cluster:
     </para>
<screen>sudo mkdir /srv/ceph/04bb24f1-d631-47ff-a2ee-22d94ad4f80c
 ceph-osd -i 12 --mkfs --mkkey
 ceph auth add osd.12 osd 'allow *' mon 'allow rwx' -i /etc/ceph/keyring.osd.12</screen>
    </step>
    <step>
     <para>
      Start the newly added OSD:
     </para>
<screen>sudo systemctl start ceph-osd@12.service</screen>
    </step>
    <step>
     <para>
      Add it to the cluster and allow replication based on &crushmap;:
     </para>
<screen>ceph osd crush set 12 osd.12 1.0 \
 pool=<replaceable>pool_name</replaceable> rack=<replaceable>rack_name</replaceable> host=<replaceable>host_name</replaceable>-osd</screen>
    </step>
    <step>
     <para>
      Check that the new OSD is in the right place within the cluster:
     </para>
<screen>ceph osd tree</screen>
    </step>
   </procedure>
   <tip>
    <para>
     The process of preparing/adding a disk can be simplified with the
     <command>ceph-disk</command> command. See
     <link xlink:href="http://ceph.com/docs/master/man/8/ceph-disk/"/> for
     more information on <command>ceph-disk</command>.
    </para>
   </tip>
  </sect2>

  <sect2 xml:id="storage.bp.disk.del">
   <title>Deleting disks</title>
   <important>
    <itemizedlist mark="bullet" spacing="normal">
     <listitem>
      <para>
       This can be done on a live cluster without downtime.
      </para>
     </listitem>
     <listitem>
      <para>
       This will cause increased replication traffic between servers.
      </para>
     </listitem>
     <listitem>
      <para>
       Be sure not to remove too many disks from your cluster to be able to
       keep the replication rules. See <xref linkend="datamgm.rules"/> for
       more information.
      </para>
     </listitem>
    </itemizedlist>
   </important>
   <para>
    To delete a disk (for example <literal>osd.12</literal>) from a &ceph;
    cluster, follow these steps:
   </para>
   <procedure>
    <step>
     <para>
      Make sure you have the right disk:
     </para>
<screen>ceph osd tree</screen>
    </step>
    <step>
     <para>
      If the disk is a member of a pool and/or active:
     </para>
     <substeps performance="required">
      <step>
       <para>
        <emphasis>Drain</emphasis> the OSD by setting its weight to zero:
       </para>
       <screen>ceph osd crush reweight osd.12 0</screen>
       <para>
        Then wait for all the placement groups to be moved away to other OSDs with <command>ceph -w</command>.
        Optionally, you can check if the OSD is emptying with <command>df -h</command>.
       </para>
      </step>
      <step>
       <para>
        Mark the disk out:
       </para>
<screen>ceph osd out 12</screen>
      </step>
      <step>
       <para>
        Stop the related OSD service:
       </para>
<screen>sudo systemctl stop ceph-osd@12.service</screen>
      </step>
     </substeps>
    </step>
    <step>
     <para>
      Remove the disk from &crushmap;:
     </para>
<screen>ceph osd crush remove osd.12</screen>
    </step>
    <step>
     <para>
      Remove authentication information for the disk:
     </para>
<screen>ceph auth del osd.12</screen>
    </step>
    <step>
     <para>
      Remove the disk from the cluster:
     </para>
<screen>ceph osd rm 12</screen>
    </step>
    <step>
     <para>
      Wipe the disk to remove all the data:
     </para>
<screen>sudo sgdisk --zap-all -- <replaceable>disk_device_name</replaceable>
sudo sgdisk --clear --mbrtogpt -- <replaceable>disk_device_name</replaceable></screen>
    </step>
   </procedure>
  </sect2>
 </sect1>
 <sect1 xml:id="storage.bp.recover">
  <title>Recovery</title>

  <para/>

  <sect2 xml:id="storage.bp.recover.toomanypgs">
   <title>'Too Many PGs per OSD' Status Message</title>
   <para>
    If you receive a <literal>Too Many PGs per OSD</literal> message after
    running <command>ceph status</command>, it means that the
    <option>mon_pg_warn_max_per_osd</option> value (300 by default) was
    exceeded. This value is compared to the number of PGs per OSD ratio.
    This means that the cluster setup is not optimal.
   </para>
   <para>
    As the number of PGs cannot be reduced after the pool is created, the
    only solution is to add OSDs to the cluster so that the ratio of PGs per
    OSD becomes lower.
   </para>
  </sect2>

  <sect2 xml:id="storage.bp.recover.stalecalamari">
   <title>Calamari Has a Stale Cluster</title>
   <para>
    The Calamari back-end supports operating multiple clusters, while its
    front-end does not yet. This means that if you point Calamari at one
    cluster, then destroy that cluster and create a new one, and then point
    the same Calamari instance at the new cluster, it will still remember
    the old cluster and possibly/probably try to display the old cluster
    state by default.
   </para>
   <para>
    To make Calamari 'forget' the old cluster, run:
   </para>
   <screen>sudo systemctl stop cthulhu.service
sudo calamari-ctl clear --yes-i-am-sure
sudo calamari-ctl initialize</screen>
   <para>
    This will make Calamari forget all the old clusters it knows about. It
    will, however, not clear out the salt minion keys from the master. This
    is fine if you are reusing the same nodes for the new cluster.
   </para>
  </sect2>

  <sect2 xml:id="storage.bp.recover.stuckinactive">
   <title>'<emphasis>nn</emphasis> pg stuck inactive' Status Message</title>
   <para>
    If you receive a <literal>stuck inactive</literal> status message after
    running <command>ceph status</command>, it means that &ceph; does not
    know where to replicate the stored data to fulfill the replication
    rules. It can happen shortly after the initial &ceph; setup and fix
    itself automatically. In other cases, this may require a manual
    interaction, such as bringing up a broken OSD, or adding a new OSD to
    the cluster. In very rare cases, reducing the replication level may
    help.
   </para>
   <para>
    If the placement groups are stuck perpetually, you need to check
    the output of <command>ceph osd tree</command>.
    The output should look tree-structured, similar to the example in 
    <xref linkend="storage.bp.recover.osddown"/>.
   </para>
   <para>
    If the output of <command>ceph osd tree</command> is rather flat as
    in the following example
   </para>
   <screen>ceph osd tree
ID WEIGHT TYPE NAME    UP/DOWN REWEIGHT PRIMARY-AFFINITY
-1      0 root default
 0      0 osd.0             up  1.00000          1.00000
 1      0 osd.1             up  1.00000          1.00000
 2      0 osd.2             up  1.00000          1.00000</screen>
<para>
 you should check that the related CRUSH map has a tree structure.
 If it is also flat, or with no hosts as in the above example, 
 it may mean that host name resolution is not working correctly across the cluster.
</para>
  </sect2>

  <sect2 xml:id="storage.bp.recover.osdweight">
   <title>OSD Weight is 0</title>
   <para>
    When OSD starts, it is assigned a weight. The higher the weight, the
    bigger the chance that the cluster writes data to the OSD. The weight is
    either specified in a cluster &crushmap;, or calculated by the OSDs'
    start-up script.
   </para>
   <para>
    In some cases, the calculated value for OSDs' weight may be rounded down
    to zero. It means that the OSD is not scheduled to store data, and no
    data is written to it. The reason is usually that the disk is too small
    (smaller than 15GB) and should be replaced with a bigger one.
   </para>
  </sect2>

  <sect2 xml:id="storage.bp.recover.osddown">
   <title>OSD is Down</title>
   <para>
    OSD daemon is either running, or stopped/down. There are 3 general
    reasons why an OSD is down:
   </para>
   <itemizedlist mark="bullet" spacing="normal">
    <listitem>
     <para>
      Hard disk failure.
     </para>
    </listitem>
    <listitem>
     <para>
      The OSD crashed.
     </para>
    </listitem>
    <listitem>
     <para>
      The server crashed.
     </para>
    </listitem>
   </itemizedlist>
   <para>
    You can see the detailed status of OSDs by running
   </para>
<screen>ceph osd tree
# id  weight  type name up/down reweight
 -1    0.02998  root default
 -2    0.009995   host doc-ceph1
 0     0.009995      osd.0 up  1 
 -3    0.009995   host doc-ceph2
 1     0.009995      osd.1 up  1 
 -4    0.009995   host doc-ceph3
 2     0.009995      osd.2 down  1</screen>
   <para>
    The example listing shows that the <literal>osd.2</literal> is down.
    Then you may check if the disk where the OSD is located is mounted:
   </para>
<screen>lsblk -f
 [...]
 vdb                      
 ├─vdb1               /var/lib/ceph/osd/ceph-2
 └─vdb2</screen>
   <para>
    You can track the reason why the OSD is down by inspecting its log file
    <filename>/var/log/ceph/ceph-osd.2.log</filename>. After you find and
    fix the reason why the OSD is not running, start it with
   </para>
<screen>sudo systemctl start ceph-osd@2.service</screen>
   <para>
    Do not forget to replace <literal>2</literal> with the actual number of
    your stopped OSD.
   </para>
  </sect2>

  <sect2 xml:id="storage.bp.recover.clockskew">
   <title>Fixing Clock Skew Errors</title>
   <para>
    The time information in all cluster nodes must be synchronized. If a
    node's time is not fully synchronized, you may get clock skew errors
    when checking the state of the cluster.
   </para>
   <para>
    Time synchronization is managed with NTP (see
    <link xlink:href="http://en.wikipedia.org/wiki/Network_Time_Protocol"/>).
    Set each node to synchronize its time with one or more NTP servers,
    preferably to the same group of NTP servers. If the time skew still
    occurs on a node, follow these steps to fix it:
   </para>
<screen>sudo rcntpd stop
 sudo rcceph stop
 sudo rcntpd start
 sudo rcceph start</screen>
   <para>
    You can then query the NTP peers and check the time offset with
    <command>sudo ntpq -p</command>.
   </para>
  </sect2>
 </sect1>
 <sect1 xml:id="storage.bp.account">
  <title>Accountancy</title>

  <para/>

  <sect2 xml:id="storage.bp.account.s3add">
   <title>Adding S3 Users</title>
   <para>
    S3 (Simple Storage Service) is an online file storage Web service,
    offered by Amazon. You can use the S3 interface to interact with the
    &ceph; &rgw;, besides the &swift; interface. You need to create
    a user to interact with the gateway.
   </para>
   <para>
    To create a user for the S3 interface, follow these steps:
   </para>
<screen>sudo radosgw-admin user create --uid=<replaceable>username</replaceable> \
 --display-name="<replaceable>display-name</replaceable>" --email=<replaceable>email</replaceable></screen>
   <para>
    For example:
   </para>
<screen>sudo radosgw-admin user create \
   --uid=example_user \
   --display-name="Example User" \
   --email=penguin@example.com</screen>
   <para>
    The command also creates the user's access and secret key. Check its
    output for <literal>access_key</literal> and
    <literal>secret_key</literal> keywords and their values:
   </para>
<screen>[...]
 "keys": [
       { "user": "example_user",
         "access_key": "11BS02LGFB6AL6H1ADMW",
         "secret_key": "vzCEkuryfn060dfee4fgQPqFrncKEIkh3ZcdOANY"}],
 [...]</screen>
  </sect2>

  <sect2 xml:id="storage.bp.account.s3rm">
   <title>Removing S3 Users</title>
   <para>
    To remove a user previously created to interact with the S3 interface,
    use the following command:
   </para>
<screen>sudo radosgw-admin user rm --uid=example_user</screen>
   <para>
    For more information on the command's options, see
    <xref linkend="storage.bp.account.swiftrm"/>.
   </para>
  </sect2>

  <sect2 xml:id="storage.bp.account.s3quota">
   <title>User Quota Management</title>
   <para>
    The &ceph; &rgw; enables you to set quotas on users and buckets
    owned by users. Quotas include the maximum number of objects in a bucket
    and the maximum storage size in megabytes.
   </para>
   <para>
    Before you enable a user quota, you first need to set its parameters:
   </para>
<screen>radosgw-admin quota set --quota-scope=user --uid=<replaceable>example_user</replaceable> \
 --max-objects=1024 --max-size=1024</screen>
   <variablelist>
    <varlistentry>
     <term><option>--max-objects</option>
     </term>
     <listitem>
      <para>
       Specifies the maximum number of objects. A negative value disables
       the check.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><option>--max-size</option>
     </term>
     <listitem>
      <para>
       Specifies the maximum number of bytes. A negative value disables the
       check.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><option>--quota-scope</option>
     </term>
     <listitem>
      <para>
       Sets the scope for the quota. The options are
       <literal>bucket</literal> and <literal>user</literal>. Bucket quotas
       apply to buckets a user owns. User quotas apply to a user.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
   <para>
    Once you set a user quota, you may enable it:
   </para>
<screen>radosgw-admin quota enable --quota-scope=user --uid=<replaceable>example_user</replaceable></screen>
   <para>
    To disable a quota:
   </para>
<screen>radosgw-admin quota disable --quota-scope=user --uid=<replaceable>example_user</replaceable></screen>
   <para>
    To list quota settings:
   </para>
<screen>radosgw-admin user info --uid=<replaceable>example_user</replaceable></screen>
   <para>
    To update quota statistics:
   </para>
<screen>radosgw-admin user stats --uid=<replaceable>example_user</replaceable> --sync-stats</screen>
  </sect2>

  <sect2 xml:id="storage.bp.account.swiftadd">
   <title>Adding &swift; Users</title>
   <para>
    &swift; is a standard for stored data access compatible with &ostack;.
    It is used to interact with the &ceph; &rgw;. You need to
    create a &swift; user, access key and secret to enable end users to
    interact with the gateway. There are two types of users: a
    <emphasis>user</emphasis> and <emphasis>subuser</emphasis>. While
    <emphasis>users</emphasis> are used when interacting with the S3
    interface, <emphasis>subusers</emphasis> are users of the &swift;
    interface. Each subuser is associated to a user.
   </para>
   <procedure>
    <step>
     <para>
      To create a &swift; user&mdash;which is a <emphasis>subuser</emphasis>
      in our terminology&mdash;you need to create the associated
      <emphasis>user</emphasis> first.
     </para>
<screen>sudo radosgw-admin user create --uid=<replaceable>username</replaceable> \
 --display-name="<replaceable>display-name</replaceable>" --email=<replaceable>email</replaceable></screen>
     <para>
      For example:
     </para>
<screen>sudo radosgw-admin user create \
   --uid=example_user \
   --display-name="Example User" \
   --email=penguin@example.com</screen>
    </step>
    <step>
     <para>
      To create a subuser (&swift; interface) for the user, you must specify
      the user ID (--uid=<replaceable>username</replaceable>), a subuser ID,
      and the access level for the subuser.
     </para>
<screen>radosgw-admin subuser create --uid=<replaceable>uid</replaceable> \
 --subuser=<replaceable>uid</replaceable> \
 --access=[ <replaceable>read | write | readwrite | full</replaceable> ]</screen>
     <para>
      For example:
     </para>
<screen>radosgw-admin subuser create --uid=example_user \
 --subuser=example_user:swift --access=full</screen>
    </step>
    <step>
     <para>
      Generate a secret key for the user.
     </para>
<screen>sudo radosgw-admin key create \
   --gen-secret \
   --subuser=example_user:swift \
   --key-type=swift</screen>
    </step>
    <step>
     <para>
      Both commands will output JSON-formatted data showing the user state.
      Notice the following lines, and remember the
      <literal>secret_key</literal> value:
     </para>
<screen>"swift_keys": [
   { "user": "example_user:swift",
     "secret_key": "r5wWIxjOCeEO7DixD1FjTLmNYIViaC6JVhi3013h"}],</screen>
    </step>
   </procedure>
   <para>
    For more information on using &swift; client, see
    <xref linkend="ceph.rgw.access"/>.
   </para>
  </sect2>

  <sect2 xml:id="storage.bp.account.swiftrm">
   <title>Removing &swift; Users</title>
   <para>
    When you remove a user, the user and subuser are removed from the
    system. However, you may remove only the subuser if you want. To remove
    a user (and subuser), specify <option>user rm</option> and the user ID.
   </para>
<screen>radosgw-admin user rm --uid=example_user</screen>
   <para>
    To remove the subuser only, specify <option>subuser rm</option> and the
    subuser ID.
   </para>
<screen>radosgw-admin subuser rm --uid=example_user:swift</screen>
   <para>
    You can make use of the following options:
   </para>
   <variablelist>
    <varlistentry>
     <term>--purge-data</term>
     <listitem>
      <para>
       Purges all data associated to the user ID.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>--purge-keys</term>
     <listitem>
      <para>
       Purges all keys associated to the user ID.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
   <tip>
    <title>Removing a Subuser</title>
    <para>
     When you remove a subuser, you are removing access to the Swift
     interface. The user will remain in the system. To remove the subuser,
     specify <option>subuser rm</option> and the subuser ID.
    </para>
<screen>sudo radosgw-admin subuser rm --uid=example_user:swift</screen>
    <para>
     You can make use of the following option:
    </para>
    <variablelist>
     <varlistentry>
      <term>--purge-keys</term>
      <listitem>
       <para>
        Purges all keys associated to the user ID.
       </para>
      </listitem>
     </varlistentry>
    </variablelist>
   </tip>
  </sect2>

  <sect2 xml:id="storage.bp.account.user_pwd">
   <title>Changing S3 and &swift; User Access and Secret Keys</title>
   <para>
    The <literal>access_key</literal> and <literal>secret_key</literal>
    parameters identify the &rgw; user when accessing the gateway. Changing
    the existing user keys is the same as creating new ones, as the old keys
    get overwritten.
   </para>
   <para>
    For S3 users, run the following:
   </para>
<screen>radosgw-admin key create --uid=<replaceable>example_user</replaceable> --key-type=s3 --gen-access-key --gen-secret</screen>
   <para>
    For &swift; users, run the following:
   </para>
<screen>radosgw-admin key create --subuser=<replaceable>example_user</replaceable>:swift --key-type=swift --gen-secret</screen>
   <variablelist>
    <varlistentry>
     <term><option>--key-type=<replaceable>type</replaceable></option>
     </term>
     <listitem>
      <para>
       Specifies the type of key. Either <literal>swift</literal> or
       <literal>s3</literal>.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><option>--gen-access-key</option>
     </term>
     <listitem>
      <para>
       Generates a random access key (for S3 user by default).
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><option>--gen-secret</option>
     </term>
     <listitem>
      <para>
       Generates a random secret key.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><option>--secret=<replaceable>key</replaceable></option>
     </term>
     <listitem>
      <para>
       Specifies a secret key, for example manually generated.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect2>
 </sect1>
 <sect1 xml:id="storage.bp.tuneups">
  <title>Tune-ups</title>

  <para/>

  <sect2 xml:id="storage.bp.tuneups.pg_num">
   <title>How Does the Number of Placement Groups Affect the Cluster Performance?</title>
   <para>
    Placement groups (PGs) are internal data structures for storing data in
    a pool across OSDs. The way &ceph; stores data into PGs is defined in a
    &crushmap;, and you can override the default by editing it. When
    creating a new pool, you need to specify the initial number of PGs for
    the pool.
   </para>
   <para>
    When your cluster is becoming 70% to 80% full, it is time to add more
    OSDs to it. When you increase the number of OSDs, you may consider
    increasing the number of PGs as well.
   </para>
   <warning>
    <para>
     Changing the number of PGs causes a lot of data transfer within the
     cluster.
    </para>
   </warning>
   <para>
    To calculate the optimal value for your newly-resized cluster is a
    complex task.
   </para>
   <para>
    A high number of PGs creates small chunks of data. This speeds up
    recovery after an OSD failure, but puts a lot of load on the monitor
    nodes as they are responsible for calculating the data location.
   </para>
   <para>
    On the other hand, a low number of PGs takes more time and data transfer
    to recover from an OSD failure, but does not impose that much load on
    monitor nodes as they need to calculate locations for less (but larger)
    data chunks.
   </para>
   <para>
    Find more information on the optimal number of PGs for your cluster
    using the <link xlink:href="http://ceph.com/pgcalc/">online
    calculator</link>.
   </para>
  </sect2>

  <sect2 xml:id="storage.bp.tuneups.mix_ssd">
   <title>Can I Use SSDs and Hard Disks on the Same Cluster?</title>
   <para>
    Solid-state drives (SSD) are generally faster than hard disks. If you
    mix the two types of disks for the same write operation, the data
    writing to the SSD disk will be slowed down by the hard disk
    performance. Thus, you should <emphasis>never mix SSDs and hard
    disks</emphasis> for data writing following <emphasis>the same
    rule</emphasis> (see <xref linkend="datamgm.rules"/> for more
    information on rules for storing data).
   </para>
   <para>
    There are generally 2 cases where using SSD and hard disk on the same
    cluster makes sense:
   </para>
   <orderedlist spacing="normal">
    <listitem>
     <para>
      Use each disk type for writing data following different rules. Then
      you need to have a separate rule for the SSD disk, and another rule
      for the hard disk.
     </para>
    </listitem>
    <listitem>
     <para>
      Use each disk type for a specific purpose. For example the SSD disk
      for journal, and the hard disk for storing data.
     </para>
    </listitem>
   </orderedlist>
  </sect2>

  <sect2 xml:id="storage.bp.tuneups.ssd_tradeoffs">
   <title>What are the Trade-offs of Using a Journal on SSD?</title>
   <para>
    Using SSDs for OSD journal(s) is better for performance as the journal
    is usually the bottleneck of hard disk-only OSDs. SSDs are often used to
    share journals of several OSDs.
   </para>
   <para>
    Following is a list of potential disadvantages of using SSDs for OSD
    journal:
   </para>
   <itemizedlist mark="bullet" spacing="normal">
    <listitem>
     <para>
      SSD disks are more expensive than hard disks. But as one OSD journal
      requires up to 6GB of disk space only, the price may not be so
      crucial.
     </para>
    </listitem>
    <listitem>
     <para>
      SSD disk consumes storage slots which can be otherwise used by a large
      hard disk to extend the cluster capacity.
     </para>
    </listitem>
    <listitem>
     <para>
      SSD disks have reduced write cycles compared to hard disks, but modern
      technologies are beginning to eliminate the problem.
     </para>
    </listitem>
    <listitem>
     <para>
      If you share more journals on the same SSD disk, you risk losing all
      the related OSDs after the SSD disk fails. This will require a lot of
      data to be moved to rebalance the cluster.
     </para>
    </listitem>
    <listitem>
     <para>
      Hotplugging disks becomes more complex as the data mapping is not 1:1
      the failed OSD and the journal disk.
     </para>
    </listitem>
   </itemizedlist>
  </sect2>
 </sect1>
 <sect1 xml:id="storage.bp.integration">
  <title>Integration</title>

  <para/>

  <sect2 xml:id="storage.bp.integration.kvm">
   <title>Storing &kvm; Disks in &ceph; Cluster</title>
   <para>
    You can create a disk image for &kvm;-driven virtual machine, store it
    in a &ceph; pool, optionally convert the content of an existing image to
    it, and then run the virtual machine with <command>qemu-kvm</command>
    making use of the disk image stored in the cluster. For more detailed
    information, see <xref linkend="cha.ceph.kvm"/>.
   </para>
  </sect2>

  <sect2 xml:id="storage.bp.integration.libvirt">
   <title>Storing &libvirt; Disks in &ceph; Cluster</title>
   <para>
    Similar to &kvm; (see <xref linkend="storage.bp.integration.kvm"/>), you
    can use &ceph; to store virtual machines driven by &libvirt;. The
    advantage is that you can run any &libvirt;-supported virtualization
    solution, such as &kvm;, &xen;, or LXC. For more information, see
    <xref linkend="cha.ceph.libvirt"/>.
   </para>
  </sect2>

  <sect2 xml:id="storage.bp.integration.xen">
   <title>Storing &xen; Disks in &ceph; Cluster</title>
   <para>
    One way to use &ceph; for storing &xen; disks is to make use of
    &libvirt; as described in <xref linkend="cha.ceph.libvirt"/>.
   </para>
   <para>
    Another option is to make &xen; talk to the <systemitem>rbd</systemitem>
    block device driver directly:
   </para>
   <procedure>
    <step>
     <para>
      If you have no disk image prepared for &xen;, create a new one:
     </para>
<screen>rbd create myimage --size 8000 --pool mypool</screen>
    </step>
    <step>
     <para>
      List images in the pool <literal>mypool</literal> and check if your
      new image is there:
     </para>
<screen>rbd list mypool</screen>
    </step>
    <step>
     <para>
      Create a new block device by mapping the <literal>myimage</literal>
      image to the <systemitem>rbd</systemitem> kernel module:
     </para>
<screen>sudo rbd map --pool mypool myimage</screen>
     <tip>
      <title>User Name and Authentication</title>
      <para>
       To specify a user name, use <option>--id
       <replaceable>user-name</replaceable></option>. Moreover, if you use
       <systemitem>cephx</systemitem> authentication, you must also specify
       a secret. It may come from a keyring or a file containing the secret:
      </para>
<screen>sudo rbd map --pool rbd myimage --id admin --keyring
 /path/to/keyring</screen>
      <para>
       or
      </para>
<screen>sudo rbd map --pool rbd myimage --id admin --keyfile /path/to/file</screen>
     </tip>
    </step>
    <step>
     <para>
      List all mapped devices:
     </para>
<screen>rbd showmapped 
 id pool   image   snap device    
 0  mypool myimage -    /dev/rbd0</screen>
    </step>
    <step>
     <para>
      Now you can configure &xen; to use this device as a disk for running a
      virtual machine. You can for example add the following line to the
      <command>xl</command>-style domain configuration file:
     </para>
<screen>disk = [ '/dev/rbd0,,sda', '/dev/cdrom,,sdc,cdrom' ]</screen>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="storage.bp.integration.mount_rbd">
   <title>Mounting and Unmounting an RBD Image</title>
   <para>
    Images stored inside a &ceph; cluster pool can be mapped to a block
    device. You can then format such device, mount it to be able to exchange
    files, and unmount it when done.
   </para>
   <procedure>
    <step>
     <para>
      Make sure your &ceph; cluster includes a pool with the disk image you
      want to mount. Assume the pool is called <literal>mypool</literal> and
      the image is <literal>myimage</literal>.
     </para>
<screen>rbd list mypool</screen>
    </step>
    <step>
     <para>
      Map the image to a new block device.
     </para>
<screen>sudo rbd map --pool mypool myimage</screen>
     <tip>
      <title>User Name and Authentication</title>
      <para>
       To specify a user name, use <option>--id
       <replaceable>user-name</replaceable></option>. Moreover, if you use
       <systemitem>cephx</systemitem> authentication, you must also specify
       a secret. It may come from a keyring or a file containing the secret:
      </para>
<screen>sudo rbd map --pool rbd myimage --id admin --keyring
 /path/to/keyring</screen>
      <para>
       or
      </para>
<screen>sudo rbd map --pool rbd myimage --id admin --keyfile /path/to/file</screen>
     </tip>
    </step>
    <step>
     <para>
      List all mapped devices:
     </para>
<screen>rbd showmapped 
 id pool   image   snap device    
 0  mypool myimage -    /dev/rbd0</screen>
     <para>
      The device we want to work on is <filename>/dev/rbd0</filename>.
     </para>
    </step>
    <step>
     <para>
      Make an XFS file system on the <filename>/dev/rbd0</filename> device.
     </para>
<screen>sudo mkfs.xfs /dev/rbd0
 log stripe unit (4194304 bytes) is too large (maximum is 256KiB)
 log stripe unit adjusted to 32KiB
 meta-data=/dev/rbd0              isize=256    agcount=9, agsize=261120 blks
          =                       sectsz=512   attr=2, projid32bit=1
          =                       crc=0        finobt=0
 data     =                       bsize=4096   blocks=2097152, imaxpct=25
          =                       sunit=1024   swidth=1024 blks
 naming   =version 2              bsize=4096   ascii-ci=0 ftype=0
 log      =internal log           bsize=4096   blocks=2560, version=2
          =                       sectsz=512   sunit=8 blks, lazy-count=1
 realtime =none                   extsz=4096   blocks=0, rtextents=0</screen>
    </step>
    <step>
     <para>
      Mount the device and check it is correctly mounted. Replace
      <filename>/mnt</filename> with your mount point.
     </para>
<screen>sudo mount /dev/rbd0 /mnt
 mount | grep rbd0
 /dev/rbd0 on /mnt type xfs (rw,relatime,attr2,inode64,sunit=8192,...</screen>
     <para>
      Now you can move data from/to the device as if it was a local
      directory.
     </para>
     <tip>
      <title>Increasing the Size of RBD Device</title>
      <para>
       If you find that the size of the RBD device is no longer enough, you
       can easily increase it.
      </para>
      <orderedlist spacing="normal">
       <listitem>
        <para>
         Increase the size of the RBD image, for example up to 10GB.
        </para>
<screen>rbd resize --size 10000  mypool/myimage
 Resizing image: 100% complete...done.</screen>
       </listitem>
       <listitem>
        <para>
         Grow the file system to fill up the new size of the device.
        </para>
<screen>sudo xfs_growfs /mnt
 [...]
 data blocks changed from 2097152 to 2560000</screen>
       </listitem>
      </orderedlist>
     </tip>
    </step>
    <step>
     <para>
      After you finish accessing the device, you can unmount it.
     </para>
<screen>sudo unmount /mnt</screen>
    </step>
   </procedure>
  </sect2>
 </sect1>
 <sect1 xml:id="storage.bp.cluster_mntc">
  <title>Cluster Maintenance</title>

  <para/>

  <sect2 xml:id="storage.bp.cluster_mntc.rados_striping">
   <title>Sending Large Objects with <command>rados</command> Fails with Full OSD</title>
   <para>
    <command>rados</command> is a command line utility to manage RADOS
    object storage. For more information, see <command>man 8
    rados</command>.
   </para>
   <para>
    If you send a large object to a &ceph; cluster with the
    <command>rados</command> utility, such as
   </para>
<screen>rados -p mypool put myobject /file/to/send</screen>
   <para>
    it can fill up all the related OSD space and cause serious trouble to
    the cluster performance. RADOS has a 'striper' API that enables
    applications to stripe large objects over multiple OSDs. If you turn the
    striping feature on with the <option>--striper</option> option, you can
    prevent the OSD from filling up.
   </para>
<screen>rados --striper -p mypool put myobject /file/to/send</screen>
  </sect2>

  <sect2 xml:id="storage.bp.cluster_mntc.calamari_addpool">
   <title>Creating and Deleting Pools from Calamari</title>
   <para>
    Apart from using the command line to create or delete pools (see
    <xref linkend="storage.bp.cluster_mntc.add_pool"/> and
    <xref linkend="storage.bp.cluster_mntc.del_pool"/>), you can do the same
    from within Calamari in a more comfortable user interface.
   </para>
   <para>
    To create a new pool using Calamari, follow these steps:
   </para>
   <procedure>
    <step>
     <para>
      Log in to a running instance of Calamari.
     </para>
    </step>
    <step>
     <para>
      Go to
      <menuchoice><guimenu>Manage</guimenu><guimenu>Pools</guimenu></menuchoice>.
      You can see a list of the cluster's existing pools.
     </para>
    </step>
    <step>
     <para>
      Click <inlinemediaobject>
      <imageobject>
       <imagedata fileref="calamari_adpool_plus.png"/>
      </imageobject>
      </inlinemediaobject> in the right top.
     </para>
    </step>
    <step>
     <para>
      Enter a name for the new pool, and either change the number of
      replicas, number of placement groups, and the CRUSH ruleset, or leave
      them at default values.
     </para>
    </step>
    <step>
     <para>
      Click <inlinemediaobject>
      <imageobject>
       <imagedata fileref="calamari_adpool_plus.png"/>
      </imageobject>
      </inlinemediaobject> to confirm, then <guimenu>Cancel</guimenu> the
      warning dialog.
     </para>
    </step>
    <step>
     <para>
      Now you can see the new pool in the list of all existing pools. You
      can verify the existence of the new pool on the command line with
     </para>
<screen>ceph osd lspools</screen>
    </step>
   </procedure>
   <para>
    To delete an existing pool using Calamari, follow these steps:
   </para>
   <procedure>
    <step>
     <para>
      Log in to a running instance of Calamari.
     </para>
    </step>
    <step>
     <para>
      Go to
      <menuchoice><guimenu>Manage</guimenu><guimenu>Pools</guimenu></menuchoice>.
      You can see a list of the cluster's existing pools.
     </para>
    </step>
    <step>
     <para>
      From the list of pools, choose the one to delete and click the related
      <inlinemediaobject>
      <imageobject>
       <imagedata fileref="calamari_rmpool_thrash.png"/>
      </imageobject>
      </inlinemediaobject>
     </para>
    </step>
    <step>
     <para>
      Confirm the deletion and <guimenu>Cancel</guimenu> the warning dialog.
     </para>
    </step>
    <step>
     <para>
      You can verify the deletion of the pool on the command line with
     </para>
<screen>ceph osd lspools</screen>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="storage.bp.cluster_mntc.mng_keyrings">
   <title>Managing Keyring Files</title>
   <para>
    When &ceph; runs with authentication and authorization enabled (enabled
    by default), you must specify a user name and a keyring containing the
    secret key of the specified user. If you do not specify a user name,
    Ceph will use <literal>client.admin</literal> as the default user name.
    If you do not specify a keyring, Ceph will look for a keyring via the
    <option>keyring</option> setting in the &ceph; configuration. For
    example, if you execute the <command>ceph health</command> command
    without specifying a user or keyring:
   </para>
<screen>ceph health</screen>
   <para>
    &ceph; interprets the command like this:
   </para>
<screen>ceph -n client.admin --keyring=/etc/ceph/ceph.client.admin.keyring health</screen>
   <para>
    <command>ceph-authtool</command> is a utility to create, view, and
    modify a &ceph; keyring file. A keyring file stores one or more &ceph;
    authentication keys and possibly an associated capability specification.
    Each key is associated with an entity name, of the form
    {client,mon,mds,osd}.name.
   </para>
   <para>
    To create a new <filename>keyring</filename> file in the current
    directory containing a key for <literal>client.example1</literal>:
   </para>
<screen>ceph-authtool -C -n client.example1 --gen-key keyring</screen>
   <para>
    To add a new key for <literal>client.example2</literal>, omit the
    <option>-C</option> option:
   </para>
<screen>ceph-authtool -n client.example2 --gen-key keyring</screen>
   <para>
    The <filename>keyring</filename> now has two entries:
   </para>
<screen>ceph-authtool -l keyring
 [client.example1]
     key = AQCQ04NV8NE3JBAAHurrwc2BTVkMGybL1DYtng==
 [client.example2]
     key = AQBv2INVWMqFIBAAf/4/H3zxzAsPBTH4jsN80w==</screen>
   <para>
    For more information on <filename>ceph-authtool</filename>, see its
    manual page <filename>man 8 ceph-authtool</filename>.
   </para>
  </sect2>

  <sect2 xml:id="storage.bp.cluster_mntc.create_client_keys">
   <title>Creating Client Keys</title>
   <para>
    User management functionality provides &ceph; cluster administrators
    with the ability to create, update and delete users directly in the
    cluster environment.
   </para>
   <tip>
    <para>
     When you create or delete users in the &ceph; cluster, you may need to
     distribute keys to clients so that they can be added to keyrings.
    </para>
   </tip>
   <para>
    Adding a user creates a user name (TYPE.ID), a secret key and possibly
    capabilities included in the command you use to create the user. A
    user’s key enables the user to authenticate with the cluster. The
    user’s capabilities authorize the user to read, write, or execute on
    monitors, OSDs, or metadata servers.
   </para>
   <para>
    Authentication key creation usually follows cluster user creation. There
    are several ways to add a user. The most convenient seems to be using
   </para>
<screen>ceph auth get-or-create</screen>
   <para>
    It returns a keyfile format with the user name [in brackets] and the
    key. If the user already exists, this command simply returns the user
    name and key in the keyfile format. You may use the<option>-o
    <replaceable>filename</replaceable></option> option to save the output
    to a file.
   </para>
<screen>ceph auth get-or-create client.example1
 [client.example1]
    key = AQDs+odVODCGGxAAvmSnsNx3XYHJ7Ri6sZFfhw==</screen>
   <para>
    You can verify that the client key was added to the cluster keyring:
   </para>
<screen>ceph auth list
    [...]
 client.example1
    key: AQDs+odVODCGGxAAvmSnsNx3XYHJ7Ri6sZFfhw==</screen>
   <para>
    When creating client users, you may create a user with no capabilities.
    A user with no capabilities is useless beyond mere authentication,
    because the client cannot retrieve the cluster map from the monitor.
    However, you can create a user with no capabilities if you want to defer
    adding capabilities later using the <command>ceph auth caps</command>
    command.
   </para>
   <tip>
    <para>
     After you add a key to the cluster keyring, go to the relevant
     client(s) and copy the keyring from the cluster host to the client(s).
    </para>
   </tip>
   <para>
    Find more details in the related upstream documentation, see
    <link xlink:href="http://ceph.com/docs/master/rados/operations/user-management/">User
    Management</link>.
   </para>
  </sect2>

  <sect2 xml:id="storage.bp.cluster_mntc.revoke_client_keys">
   <title>Revoking Client Keys</title>
   <para>
    If you need to remove an already generated client key from the keyring
    file, use the <command>ceph auth del</command> command. To remove the
    key for user <literal>client.example1</literal> that we added in
    <xref linkend="storage.bp.cluster_mntc.create_client_keys"/>:
   </para>
<screen>ceph auth del client.example1</screen>
   <para>
    and check the deletion with <command>ceph auth list</command>.
   </para>
   <tip>
    <para>
     After you add a key to the cluster keyring, go to the relevant
     client(s) and copy the keyring from the cluster host to the client(s).
    </para>
   </tip>
  </sect2>

  <sect2 xml:id="storage.bp.cluster_mntc.unbalanced">
   <title>Checking for Unbalanced Data Writing</title>
   <para>
    When data is written to OSDs evenly, the cluster is considered balanced.
    Each OSD within a cluster is assigned its <emphasis>weight</emphasis>.
    The weight is a relative number and tells &ceph; how much of the data
    should be written to the related OSD. The higher the weight, the more
    data will be written. If an OSD has zero weight, no data will be written
    to it. If the weight of an OSD is relatively high compared to other
    OSDs, a large portion of the data will be written there, which makes the
    cluster unbalanced.
   </para>
   <para>
    Unbalanced clusters have poor performance, and in the case that an OSD
    with a high weight suddenly crashes, a lot of data needs to be moved to
    other OSDs, which slows down the cluster as well.
   </para>
   <para>
    To avoid this, you should regularly check OSDs for the amount of data
    writing. If the amount is between 30% and 50% of the capacity of a group
    of OSDs specified by a given rule set, you need to reweight the OSDs.
    Check for individual disks and find out which of them fill up faster
    than the others (or are generally slower), and lower their weight. The
    same is valid for OSDs where not enough data is written&mdash;you can
    increase their weight to have &ceph; write more data to them. In the
    following example, you will find out the weight of an OSD with ID 13,
    and reweight it from 3 to 3.05:
   </para>
<screen>$ ceph osd tree | grep osd.13
 13  3                   osd.13  up  1 

 $ ceph osd crush reweight osd.13 3.05
 reweighted item id 13 name 'osd.13' to 3.05 in crush map

 $ ceph osd tree | grep osd.13
 13  3.05                osd.13  up  1</screen>
   <para/>
   <tip>
    <title>OSD Reweight by Utilization</title>
    <para>
     The <command>ceph osd reweight-by-utilization</command>
     <replaceable>threshold</replaceable> command automates the process of
     reducing the weight of OSDs which are heavily overused. By default it
     will adjust the weights downward on OSDs which reached 120% of the
     average utilization, but if you include threshold it will use that
     percentage instead.
    </para>
   </tip>
  </sect2>

  <sect2 xml:id="storage.bp.cluster_mntc.sw_upg">
   <title>Upgrading Software</title>
   <para>
    Both &sls; and &storage; products are provided with regular package
    updates. To apply new updates to the whole cluster, you need to run
   </para>
<screen>sudo zypper dup</screen>
   <para>
    on all cluster nodes. Remember to upgrade all the monitor nodes first,
    and then all the OSD nodes one by one.
   </para>
  </sect2>

  <sect2 xml:id="storage.bp.cluster_mntc.add_pgnum">
   <title>Increasing the Number of Placement Groups</title>
   <para>
    When creating a new pool, you specify the number of placement groups for
    the pool (see <xref linkend="ceph.pools.operate.add_pool"/>). After
    adding more OSDs to the cluster, you usually need to increase the number
    of placement groups as well for performance and data durability reasons.
    For each placement group, OSD and monitor nodes need memory, network and
    CPU at all times and even more during recovery. From which follows that
    minimizing the number of placement groups saves significant amounts of
    resources.
   </para>
   <warning>
    <title>Too High Value of <option>pg_num</option></title>
    <para>
     When changing the <option>pg_num</option> value for a pool, it may
     happen that the new number of placement groups exceeds the allowed
     limit. For example
    </para>
<screen>ceph osd pool set rbd pg_num 4096
 Error E2BIG: specified pg_num 3500 is too large (creating 4096 new PGs \
 on ~64 OSDs exceeds per-OSD max of 32)</screen>
    <para>
     The limit prevents extreme placement group splitting, and is derived
     from the <option>mon_osd_max_split_count</option> value.
    </para>
   </warning>
   <para>
    To determine the right new number of placement groups for a resized
    cluster is a complex task. One approach is to continuously grow the
    number of placement groups up to the state when the cluster performance
    is optimal. To determine the new incremented number of placement groups,
    you need to get the value of the
    <option>mon_osd_max_split_count</option> parameter, and add it to the
    current number of placement groups. To give you a basic idea, take a
    look at the following script:
   </para>
<screen>max_inc=`ceph daemon mon.a config get mon_osd_max_split_count 2&gt;&amp;1 \
  | tr -d '\n ' | sed 's/.*"\([[:digit:]]\+\)".*/\1/'`
 pg_num=`ceph osd pool get rbd pg_num | cut -f2 -d: | tr -d ' '`
 echo "current pg_num value: $pg_num, max increment: $max_inc"
 next_pg_num="$(($pg_num+$max_inc))"
 echo "allowed increment of pg_num: $next_pg_num"</screen>
   <para>
    After finding out the next number of placement groups, increase it with
   </para>
<screen>ceph osd pool set <replaceable>pool_name</replaceable> pg_num <replaceable>next_pg_num</replaceable></screen>
  </sect2>

  <sect2 xml:id="storage.bp.cluster_mntc.add_pool">
   <title>Adding a Pool</title>
   <para>
    After you first deploy a cluster, &ceph; uses the default pools to store
    data. You can later create a new pool with
   </para>
<screen>ceph osd pool create</screen>
   <para>
    For more information on cluster pool creation, see
    <xref linkend="ceph.pools.operate.add_pool"/>.
   </para>
  </sect2>

  <sect2 xml:id="storage.bp.cluster_mntc.del_pool">
   <title>Deleting a Pool</title>
   <para>
    By deleting a pool, you permanently destroy all data stored in that
    pool. You can delete a previously created pool with
   </para>
<screen>ceph osd pool delete</screen>
   <para>
    For more information on cluster pool deletion, see
    <xref linkend="ceph.pools.operate.del_pool"/>.
   </para>
  </sect2>
 </sect1>
 <sect1 xml:id="storage.bp.performance">
  <title>Performance Diagnosis</title>

  <para/>

  <sect2 xml:id="storage.bp.performance.slowosd">
   <title>Finding Slow OSDs</title>
   <para>
    When tuning the cluster performance, it is very important to identify
    slow storage/OSDs within the cluster. The reason is that if the data is
    written to the slow(est) disk, the complete write operation slows down
    as it always waits until it is finished on all the related disks.
   </para>
   <para>
    It is not trivial to locate the storage bottleneck. You need to examine
    each and every OSD to find out the ones slowing down the write process.
    To do a benchmark on a single OSD, run:
   </para>
<screen role="ceph_tell_osd_bench"><command>ceph tell</command> <replaceable>osd_id</replaceable> bench</screen>
   <para>
    For example:
   </para>
<screen>&prompt.cephuser;ceph tell osd.0 bench
 { "bytes_written": 1073741824,
   "blocksize": 4194304,
   "bytes_per_sec": "19377779.000000"}</screen>
   <para>
    Then you need to run this command on each OSD and compare the
    <literal>bytes_per_sec</literal> value to get the slow(est) OSDs.
   </para>
  </sect2>

  <sect2 xml:id="storage.bp.performance.net_issues">
   <title>Is My Network Causing Issues?</title>
   <para>
    There are more reasons why the cluster performance may become weak. One
    of them can be network problems. In such case, you may notice the
    cluster reaching quorum, OSD and monitor nodes going offline, data
    transfers taking a long time, or a lot of reconnect attempts.
   </para>
   <para>
    To check whether cluster performance is degraded by network problems,
    inspect the &ceph; log files under the
    <filename>/var/log/ceph</filename> directory.
   </para>
   <para>
    To fix network issues on the cluster, focus on the following points:
   </para>
   <itemizedlist mark="bullet" spacing="normal">
    <listitem>
     <para>
      Basic network diagnostics. Try to ping between cluster nodes and pay
      attention to data loss and response times.
     </para>
    </listitem>
    <listitem>
     <para>
      Network performance benchmark. Use tools such as Netperf to measure
      the performance of your network.
     </para>
    </listitem>
    <listitem>
     <para>
      Check firewall settings on cluster nodes. Make sure they do not block
      ports/protocols required by &ceph; operation. See
      <xref linkend="storage.bp.net.firewall"/> for more information on
      firewall settings.
     </para>
    </listitem>
    <listitem>
     <para>
      Check the networking hardware, such as network cards, cables, or
      switches, for proper operation.
     </para>
    </listitem>
   </itemizedlist>
   <tip>
    <title>Separate Network</title>
    <para>
     To ensure fast and safe network communication between cluster nodes,
     set up a separate network used exclusively by the cluster OSD and
     monitor nodes.
    </para>
   </tip>
  </sect2>
 </sect1>
 <sect1 xml:id="storage.bp.srv_maint">
  <title>Server Maintenance</title>

  <para/>

  <sect2 xml:id="storage.bp.srv_maint.add_server">
   <title>Adding a Server to a Cluster</title>
   <tip>
    <para>
     When adding an OSD to an existing cluster, be aware that the cluster
     will be rebalancing for some time afterwards. To minimize the
     rebalancing periods, it is best to add all the OSDs you intend to add
     at the same time.
    </para>
   </tip>
   <para>
    If you are adding an OSD to a cluster, follow
    <xref linkend="storage.bp.inst.add_osd_cephdeploy"/>.
   </para>
   <para>
    If you are adding a monitor to a cluster, follow
    <xref linkend="storage.bp.inst.add_rm_monitor"/>.
   </para>
   <important>
    <para>
     After adding a monitor, make sure that
     <filename>/etc/ceph/ceph.conf</filename> files on each server point to
     the new monitor as well so that it works after the next reboot.
    </para>
   </important>
   <tip>
    <para>
     Adding an OSD and monitor on the same server is recommended only for
     small size clusters. Although the monitor can share disk with the
     operating system (preferably an SSD disk for performance reasons), it
     should <emphasis>never</emphasis> share disk with an OSD.
    </para>
   </tip>
  </sect2>

  <sect2 xml:id="storage.bp.srv_maint.rm_server">
   <title>Removing a Server from a Cluster</title>
   <para>
    When removing an OSD from an existing cluster, make sure there are
    enough OSDs left in the cluster so that the replication rules can be
    followed. Also be aware that the cluster will be rebalancing for some
    time after removing the OSD.
   </para>
   <para>
    If you are removing an OSD from a cluster, follow
    <xref linkend="storage.bp.disk.del"/>.
   </para>
   <para>
    If you are removing a monitor from a cluster, follow
    <xref linkend="storage.bp.inst.add_rm_monitor.rmmon"/>.
   </para>
  </sect2>

  <sect2 xml:id="storage.bp.srv_maint.fds_inc">
   <title>Increasing File Descriptors</title>
   <para>
    For OSD daemons, the read/write operations are critical to keep the
    &ceph; cluster balanced. They often need to have many files open for
    reading and writing at the same time. On the OS level, the maximum
    number of simultaneously open files is called 'maximum number of file
    descriptors'.
   </para>
   <para>
    To prevent OSDs from running out of file descriptors, you can override
    the OS default value and specify the number in
    <filename>/etc/ceph/ceph.conf</filename>, for example:
   </para>
<screen>max_open_files = 131072</screen>
   <para>
    After you change <option>max_open_files</option>, you need to restart
    the OSD service on the relevant &ceph; node.
   </para>
  </sect2>
 </sect1>
 <sect1 xml:id="storage.bp.net">
  <title>Networking</title>

  <para/>

  <sect2 xml:id="storage.bp.net.ntp">
   <title>Setting NTP to a &ceph; Cluster</title>
   <para>
    In a cluster environment, it is necessary to keep all cluster nodes'
    time synchronized. NTP&mdash;Network Time Protocol&mdash;is a network
    service commonly used for this purpose. NTP is well integrated in &suse;
    products, including &storage;. There are two ways to configure
    NTP&mdash;either using &yast;, or setting it up manually. Find both
    methods described&mdash;and more information on NTP in general&mdash;in
    <link xlink:href="https://www.suse.com/documentation/sles-12/book_sle_admin/data/cha_netz_xntp.html">&sls;
    Administration Guide</link>.
   </para>
  </sect2>

  <sect2 xml:id="storage.bp.net.firewall">
   <title>Firewall Settings for &ceph;</title>
   <para>
    We recommend protecting the network cluster communication with SUSE
    Firewall. You can edit its configuration by selecting
    <menuchoice><guimenu>&yast;</guimenu><guimenu>Security and
    Users</guimenu><guimenu>Firewall</guimenu><guimenu>Allowed
    Services</guimenu></menuchoice>.
   </para>
   <para>
    For Calamari, enable the "HTTP Server", "Carbon" and "SaltStack"
    services (ports 80, 2003, 2004, 4505 and 4506).
   </para>
   <para>
    For &ceph; monitor nodes, enable the "Ceph MON" service (port 6789).
   </para>
   <para>
    For &ceph; OSD (or MDS) nodes, enable the "Ceph OSD/MDS" service (ports
    6800-7300).
   </para>
  </sect2>

  <sect2 xml:id="storage.bp.net.private">
   <title>Adding a Private Network to a Running Cluster</title>
   <para>
    If you do not specify a cluster network during &ceph; deployment, it
    assumes a single public network environment. While &ceph; operates fine
    with a public network, its performance and security improves when you
    set a second private cluster network.
   </para>
   <para>
    A general recommendation for a &ceph; cluster is to have two networks: a
    public (front-side) and cluster (back-side) one. To support two
    networks, each &ceph; node needs to have at least two network cards.
   </para>
   <para>
    You need to apply the following changes to each &ceph; node. It is
    comfortable for a small cluster, but can be very time demanding if you
    have a cluster consisting of hundreds or thousands of nodes.
   </para>
   <procedure>
    <step>
     <para>
      Stop &ceph; related services on each cluster node.
     </para>
     <para>
      Replace <literal>10.0.0.0/24</literal> with the IP address and netmask
      of the cluster network. You can specify more comma-delimited subnets.
      If you need to specifically assign static IP addresses or override
      <option>cluster network</option> settings, you can do so with the
      optional <option>cluster addr</option>.
     </para>
    </step>
    <step>
     <para>
      Check that the private cluster network works as expected on the OS
      level.
     </para>
    </step>
    <step>
     <para>
      Start &ceph; related services on each cluster node.
     </para>
<screen>sudo rcceph start</screen>
    </step>
   </procedure>
  </sect2>
 </sect1>
</chapter>
