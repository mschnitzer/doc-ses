<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<!-- Converted by suse-upgrade version 1.1 -->
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="cha.ceph.operating">
 <title>Operating a &ceph; Cluster</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:maintainer>tbazant@suse.com</dm:maintainer>
   <dm:status>editing</dm:status>
   <dm:deadline></dm:deadline>
   <dm:priority></dm:priority>
   <dm:translation></dm:translation>
   <dm:languages></dm:languages>
   <dm:release>SES2.1</dm:release>
  </dm:docmanager>
 </info>
 <para>
  This section introduces information on basic cluster services operating.
  You will learn how to start or stop &ceph; services, how to monitor a
  cluster's state, how to use and modify CRUSH maps, and how to manage
  storage pools.
 </para>
 <sect1 xml:id="ceph.operating.services">
  <title>Operating &ceph; Services</title>

  <para>
   &ceph; related services are operated with the
   <command>systemctl</command> command. The operation takes place on the
   node you are currently logged in to. You need to have &rootuser;
   privileges to be able to operate on &ceph; services.
  </para>

  <para>
   The following subcommands are supported for all &ceph; services:
  </para>

  <variablelist>
   <varlistentry>
    <term>systemctl status <replaceable>target</replaceable>
    </term>
    <listitem>
     <para>
      Prints the status information of the service(s) specified by
      <replaceable>target</replaceable>.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>systemctl start <replaceable>target</replaceable>
    </term>
    <listitem>
     <para>
      Starts the service(s) specified by <replaceable>target</replaceable>
      if they are not already running.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>systemctl stop <replaceable>target</replaceable>
    </term>
    <listitem>
     <para>
      Stops the running service(s) specified by
      <replaceable>target</replaceable>.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>systemctl enable <replaceable>target</replaceable>
    </term>
    <listitem>
     <para>
      Enables the service(s) specified by <replaceable>target</replaceable>
      so that they are automatically started on system start-up.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>systemctl disable <replaceable>target</replaceable>
    </term>
    <listitem>
     <para>
      Disables the service(s) specified by <replaceable>target</replaceable>
      so that they are not automatically started on system start-up. You
      need to start them manually with <command>systemctl start</command>.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <para>
   The following subcommand is supported for OSD related targets:
  </para>

  <variablelist>
   <varlistentry>
    <term>systemctl mask <replaceable>target</replaceable>
    </term>
    <listitem>
     <para>
      Prevents the service from being started manually or automatically.
      <command>systemctl</command> is creating a symbolic link from the
      specific target file located in
      <filename>/etc/systemd/system/</filename> to
      <filename>/dev/null</filename>. Targets in
      <filename>/etc/systemd</filename> override those provided by packages
      in <filename>/usr/lib/systemd</filename>. &systemd; recognizes the
      symbolic link and will not start the service.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>systemctl unmask <replaceable>target</replaceable>
    </term>
    <listitem>
     <para>
      Opposite to <command>systemctl mask</command>. Removes the symbolic
      link from the specific target file located in
      <filename>/etc/systemd/system/</filename> to
      <filename>/dev/null</filename>, so that targets located in
      <filename>/usr/lib/systemd</filename> are valid again.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <para>
   You can either operate on all &ceph; services at once, or all &ceph;
   services of a certain type (such as OSD or monitor), or a specific
   instance of service identified by its instance name.
  </para>

  <sect2>
   <title>Operating on All &ceph; Services</title>
   <para>
    To operate on all &ceph; services at once, run:
   </para>
<screen>systemctl <replaceable>subcommand</replaceable> ceph*</screen>
   <para>
    For example:
   </para>
<screen>systemctl status ceph*
ceph-osd@0.service - Ceph object storage daemon
    Loaded: loaded (/usr/lib/systemd/system/ceph-osd@.service; enabled)
    Active: active (running) since Fri 2015-01-23 09:09:12 EST; 4min 52s ago
  Main PID: 2788 (ceph-osd)
    CGroup:
/system.slice/system-ceph\x2dosd.slice/ceph-osd@0.service
            └─2788 /usr/bin/ceph-osd -f \-\-cluster ceph \-\-id 0
Jan 23 09:09:12 ceph-node2 ceph-osd-prestart.sh[2737]:
create-or-move updated item name 'osd.0' weight 0.01 at location
{host=ceph-node2,root=default} to crush map
 Jan 23 09:09:12 ceph-node2 ceph-osd[2788]: starting osd.0 at
:/0 osd_data /var/lib/ceph/osd/ceph-0 /var/lib/ceph/osd/ceph-0/journal
 Jan 23 09:09:12 ceph-node2 ceph-osd[2788]:
HDIO_DRIVE_CMD(identify) failed: Inappropriate ioctl for device
 Jan 23 09:09:12 ceph-node2 ceph-osd[2788]:
HDIO_DRIVE_CMD(identify) failed: Inappropriate ioctl for device

ceph-radosgw@rgw.ceph4.service - Ceph rados gateway
    Loaded: loaded
(/usr/lib/systemd/system/ceph-radosgw@rgw.service; enabled)
    Active: active (running) since Fri 2015-01-23 09:11:29
EST; 2min 35s ago
  Main PID: 3741 (radosgw)
    CGroup:
/system.slice/system-ceph\x2dradosgw.slice/ceph-radosgw@rgw.ceph4.service
            └─3741 /usr/bin/radosgw -f \-\-conf
/etc/ceph/ceph.conf \-\-name client.radosgw.ceph4
 Jan 23 09:11:31 ceph-node2 radosgw[3741]: 2015-01-23
09:11:31.206129 7f60e1df9700 -1 failed to list objects pool_iterate returned r=-2</screen>
   <tip>
    <para>
     <command>systemctl</command> &ceph; commands cannot operate on services
     not enabled, not masked, and those which have never been started. To
     include all present services on a node, use the
     <command>rcceph</command> wrapper. For example:
    </para>
<screen>rcceph status</screen>
   </tip>
  </sect2>

  <sect2>
   <title>Operating on All &ceph; Services of a Specific Type</title>
   <para>
    It is possible to address services of a specific type only&mdash;such as
    OSDs, monitors, or &rgw;:
   </para>
<screen>systemctl <replaceable>subcommand</replaceable> ceph-<replaceable>service_type</replaceable>*</screen>
   <para>
    A few practical examples follow:
   </para>
   <para>
    Print the status of all monitor services on a node:
   </para>
<screen>systemctl status ceph-mon*</screen>
   <para>
    Start all OSD services on a node:
   </para>
<screen>systemctl start ceph-osd*</screen>
   <para>
    Stop all &rgw; services on a node:
   </para>
<screen>systemctl stop ceph-radosgw*</screen>
  </sect2>

  <sect2>
   <title>Operating on a Specific &ceph; Service</title>
   <para>
    If you want to operate on a single service on a node, you need to know
    its instance name. It is available in the output of <command>systemctl
    status ceph*</command>.
   </para>
<screen>systemctl <replaceable>subcommand</replaceable> ceph-<replaceable>service_type</replaceable>@<replaceable>instance_name</replaceable>.service</screen>
   <para>
    Print the status information of the OSD service with instance name
    <literal>0</literal>:
   </para>
<screen>systemctl status ceph-osd@0.service</screen>
  </sect2>
 </sect1>
 <sect1 xml:id="ceph.monitor">
  <title>Determining Cluster State</title>

<!-- 2014-01-13 tbazant: reusing http://docs.ceph.com/docs/master/rados/operations/monitoring/ -->

  <para>
   Once you have a running cluster, you may use the <command>ceph</command>
   tool to monitor your cluster. Determining cluster state typically involves
   checking OSD status, monitor status, placement group status and metadata
   server status.
  </para>

  <tip>
   <title>Interactive Mode</title>
   <para>
    To run the <command>ceph</command> tool in an interactive mode, type
    <command>ceph</command> at the command line with no arguments. The
    interactive mode is more convenient if you are going to enter more
    <command>ceph</command> commands in a row. For example:
   </para>
<screen>ceph
ceph&gt; health
ceph&gt; status
ceph&gt; quorum_status
ceph&gt; mon_status</screen>
  </tip>

  <sect2 xml:id="monitor.health">
   <title>Checking Cluster Health</title>
   <para>
    After you start your cluster, and before you start reading and/or
    writing data, check your cluster’s health first. You can check on the
    health of your &ceph; cluster with the following:
   </para>
<screen>ceph health
HEALTH_WARN 10 pgs degraded; 100 pgs stuck unclean; 1 mons down, quorum 0,2 \
node-1,node-2,node-3</screen>
   <para>
    If you specified non-default locations for your configuration or
    keyring, you may specify their locations:
   </para>
<screen>ceph -c <replaceable>/path/to/conf</replaceable> -k <replaceable>/path/to/keyring</replaceable> health</screen>
   <para>
    Upon starting the &ceph; cluster, you will likely encounter a health
    warning such as <literal>HEALTH_WARN XXX num placement groups
    stale</literal>. Wait a few moments and check it again. When your
    cluster is ready, <command>ceph health</command> should return a message
    such as <literal>HEALTH_OK</literal>. At that point, it is okay to begin
    using the cluster.
   </para>
  </sect2>

  <sect2 xml:id="monitor.watch">
   <title>Watching a Cluster</title>
   <para>
    To watch the cluster’s ongoing events, open a new terminal and enter:
   </para>
<screen>ceph -w</screen>
   <para>
    &ceph; will print each event. For example, a tiny &ceph; cluster
    consisting of one monitor, and two OSDs may print the following:
   </para>
<screen>cluster b370a29d-9287-4ca3-ab57-3d824f65e339
 health HEALTH_OK
 monmap e1: 1 mons at {ceph1=10.0.0.8:6789/0}, election epoch 2, quorum 0 ceph1
 osdmap e63: 2 osds: 2 up, 2 in
  pgmap v41338: 952 pgs, 20 pools, 17130 MB data, 2199 objects
        115 GB used, 167 GB / 297 GB avail
             952 active+clean

2014-06-02 15:45:21.655871 osd.0 [INF] 17.71 deep-scrub ok
2014-06-02 15:45:47.880608 osd.1 [INF] 1.0 scrub ok
2014-06-02 15:45:48.865375 osd.1 [INF] 1.3 scrub ok
2014-06-02 15:45:50.866479 osd.1 [INF] 1.4 scrub ok
[...]
2014-06-02 15:45:55.720929 mon.0 [INF] pgmap v41343: 952 pgs: \
 1 active+clean+scrubbing+deep, 951 active+clean; 17130 MB data, 115 GB used, \
 167 GB / 297 GB avail</screen>
   <para>
    The output provides the following information:
   </para>
   <itemizedlist mark="bullet" spacing="normal">
    <listitem>
     <para>
      Cluster ID
     </para>
    </listitem>
    <listitem>
     <para>
      Cluster health status
     </para>
    </listitem>
    <listitem>
     <para>
      The monitor map epoch and the status of the monitor quorum
     </para>
    </listitem>
    <listitem>
     <para>
      The OSD map epoch and the status of OSDs
     </para>
    </listitem>
    <listitem>
     <para>
      The placement group map version
     </para>
    </listitem>
    <listitem>
     <para>
      The number of placement groups and pools
     </para>
    </listitem>
    <listitem>
     <para>
      The <emphasis>notional</emphasis> amount of data stored and the number
      of objects stored; and,
     </para>
    </listitem>
    <listitem>
     <para>
      The total amount of data stored.
     </para>
    </listitem>
   </itemizedlist>
   <tip>
    <title>How &ceph; Calculates Data Usage</title>
    <para>
     The <literal>used</literal> value reflects the actual amount of raw
     storage used. The <literal>xxx GB / xxx GB</literal> value means the
     amount available (the lesser number) of the overall storage capacity of
     the cluster. The notional number reflects the size of the stored data
     before it is replicated, cloned or snapshotted. Therefore, the amount
     of data actually stored typically exceeds the notional amount stored,
     because &ceph; creates replicas of the data and may also use storage
     capacity for cloning and snapshotting.
    </para>
   </tip>
  </sect2>

  <sect2 xml:id="monitor.stats">
   <title>Checking a Cluster’s Usage Stats</title>
   <para>
    To check a cluster’s data usage and data distribution among pools, you
    can use the <command>df</command> option. It is similar to Linux
    <command>df</command>. Execute the following:
   </para>
<screen>ceph df
GLOBAL:
    SIZE       AVAIL      RAW USED     %RAW USED 
    27570M     27304M         266M          0.97 
POOLS:
    NAME             ID     USED     %USED     MAX AVAIL     OBJECTS 
    data             0       120         0         5064M           4 
    metadata         1         0         0         5064M           0 
    rbd              2         0         0         5064M           0 
    hot-storage      4       134         0         4033M           2 
    cold-storage     5      227k         0         5064M           1 
    pool1            6         0         0         5064M           0</screen>
   <para>
    The <literal>GLOBAL</literal> section of the output provides an overview
    of the amount of storage your cluster uses for your data.
   </para>
   <itemizedlist mark="bullet" spacing="normal">
    <listitem>
     <para>
      <literal>SIZE</literal>: The overall storage capacity of the cluster.
     </para>
    </listitem>
    <listitem>
     <para>
      <literal>AVAIL</literal>: The amount of free space available in the
      cluster.
     </para>
    </listitem>
    <listitem>
     <para>
      <literal>RAW USED</literal>: The amount of raw storage used.
     </para>
    </listitem>
    <listitem>
     <para>
      <literal>% RAW USED</literal>: The percentage of raw storage used. Use
      this number in conjunction with the <literal>full ratio</literal> and
      <literal>near full ratio</literal> to ensure that you are not reaching
      your cluster’s capacity. See
      <link xlink:href="http://docs.ceph.com/docs/master/rados/configuration/mon-config-ref#storage-capacit">Storage
      Capacity</link> for additional details.
     </para>
    </listitem>
   </itemizedlist>
   <para>
    The <literal>POOLS</literal> section of the output provides a list of
    pools and the notional usage of each pool. The output from this section
    <emphasis>does not</emphasis> reflect replicas, clones or snapshots. For
    example, if you store an object with 1MB of data, the notional usage
    will be 1MB, but the actual usage may be 2MB or more depending on the
    number of replicas, clones and snapshots.
   </para>
   <itemizedlist mark="bullet" spacing="normal">
    <listitem>
     <para>
      <literal>NAME</literal>: The name of the pool.
     </para>
    </listitem>
    <listitem>
     <para>
      <literal>ID</literal>: The pool ID.
     </para>
    </listitem>
    <listitem>
     <para>
      <literal>USED</literal>: The notional amount of data stored in
      kilobytes, unless the number appends M for megabytes or G for
      gigabytes.
     </para>
    </listitem>
    <listitem>
     <para>
      <literal>%USED</literal>: The notional percentage of storage used per
      pool.
     </para>
    </listitem>
    <listitem>
     <para>
      <literal>OBJECTS</literal>: The notional number of objects stored per
      pool.
     </para>
    </listitem>
   </itemizedlist>
   <note>
    <para>
     The numbers in the POOLS section are notional. They are not inclusive
     of the number of replicas, snapshots or clones. As a result, the sum of
     the USED and %USED amounts will not add up to the RAW USED and %RAW
     USED amounts in the %GLOBAL section of the output.
    </para>
   </note>
  </sect2>

  <sect2 xml:id="monitor.status">
   <title>Checking a Cluster’s Status</title>
   <para>
    To check a cluster’s status, execute the following:
   </para>
<screen>ceph status</screen>
   <para>
    or
   </para>
<screen>ceph -s</screen>
   <para>
    In interactive mode, type <command>status</command> and press
    <keycap function="enter"/>.
   </para>
<screen>ceph&gt; status</screen>
   <para>
    &ceph; will print the cluster status. For example, a tiny &ceph; cluster
    consisting of one monitor and two OSDs may print the following:
   </para>
<screen>cluster b370a29d-9287-4ca3-ab57-3d824f65e339
 health HEALTH_OK
 monmap e1: 1 mons at {ceph1=10.0.0.8:6789/0}, election epoch 2, quorum 0 ceph1
 osdmap e63: 2 osds: 2 up, 2 in
  pgmap v41332: 952 pgs, 20 pools, 17130 MB data, 2199 objects
        115 GB used, 167 GB / 297 GB avail
               1 active+clean+scrubbing+deep
             951 active+clean</screen>
  </sect2>

  <sect2 xml:id="monitor.osdstatus">
   <title>Checking OSD Status</title>
   <para>
    You can check OSDs to ensure they are up and on by executing:
   </para>
<screen>ceph osd stat</screen>
   <para>
    or
   </para>
<screen>ceph osd dump</screen>
   <para>
    You can also view OSDs according to their position in the CRUSH map.
   </para>
<screen>ceph osd tree</screen>
   <para>
    &ceph; will print out a CRUSH tree with a host, its OSDs, whether they
    are up and their weight.
   </para>
<screen># id    weight  type name       up/down reweight
-1      3       pool default
-3      3               rack mainrack
-2      3                       host osd-host
0       1                               osd.0   up      1
1       1                               osd.1   up      1
2       1                               osd.2   up      1</screen>
  </sect2>

  <sect2 xml:id="monitor.monstatus">
   <title>Checking Monitor Status</title>
   <para>
    If your cluster has multiple monitors (likely), you should check the
    monitor quorum status after you start the cluster before reading and/or
    writing data. A quorum must be present when multiple monitors are
    running. You should also check monitor status periodically to ensure
    that they are running.
   </para>
   <para>
    To display the monitor map, execute the following:
   </para>
<screen>ceph mon stat</screen>
   <para>
    or
   </para>
<screen>ceph mon dump</screen>
   <para>
    To check the quorum status for the monitor cluster, execute the
    following:
   </para>
<screen>ceph quorum_status</screen>
   <para>
    &ceph; will return the quorum status. For example, a &ceph; cluster
    consisting of three monitors may return the following:
   </para>
<screen>{ "election_epoch": 10,
  "quorum": [
        0,
        1,
        2],
  "monmap": { "epoch": 1,
      "fsid": "444b489c-4f16-4b75-83f0-cb8097468898",
      "modified": "2011-12-12 13:28:27.505520",
      "created": "2011-12-12 13:28:27.505520",
      "mons": [
            { "rank": 0,
              "name": "a",
              "addr": "127.0.0.1:6789\/0"},
            { "rank": 1,
              "name": "b",
              "addr": "127.0.0.1:6790\/0"},
            { "rank": 2,
              "name": "c",
              "addr": "127.0.0.1:6791\/0"}
           ]
    }
}</screen>
  </sect2>
  <!-- 2015-11-27 tbazant: MDS not supported yet
  <sect2 xml:id="monitor.mdsstatus">
   <title>Checking MDS Status</title>
   <para>
    Metadata servers provide metadata services for &ceph; FS. Metadata
    servers have two sets of states: <literal>up | down</literal> and
    <literal>active | inactive</literal>. To ensure your metadata servers
    are <literal>up</literal> and <literal>active</literal>, execute the
    following:
   </para>
<screen>ceph mds stat</screen>
   <para>
    To display details of the metadata cluster, execute the following:
   </para>
<screen>ceph mds dump</screen>
  </sect2>
  -->

  <sect2 xml:id="monitor.pgroupstatus">
   <title>Checking Placement Group States</title>
   <para>
    Placement groups map objects to OSDs. When you monitor your placement
    groups, you will want them to be <literal>active</literal> and
    <literal>clean</literal>. For a detailed discussion, refer to
    <link xlink:href="http://docs.ceph.com/docs/master/rados/operations/monitoring-osd-pg">Monitoring
    OSDs and Placement Groups.</link>
   </para>
  </sect2>

  <sect2 xml:id="monitor.adminsocket">
   <title>Using the Admin Socket</title>
   <para>
    The &ceph; admin socket allows you to query a daemon via a socket
    interface. By default, &ceph; sockets reside under
    <filename>/var/run/ceph</filename>. To access a daemon via the admin
    socket, log in to the host running the daemon and use the following
    command:
   </para>
<screen>ceph --admin-daemon /var/run/ceph/<replaceable>socket-name</replaceable></screen>
   <para>
    To view the available admin socket commands, execute the following
    command:
   </para>
<screen>ceph --admin-daemon /var/run/ceph/<replaceable>socket-name</replaceable> help</screen>
   <para>
    The admin socket command enables you to show and set your configuration
    at runtime. See
    <remark>add xref to
  http://docs.ceph.com/docs/master/rados/configuration/ceph-conf#ceph-runtime-config</remark>
    Viewing a Configuration at Runtime for details.
   </para>
   <para>
    Additionally, you can set configuration values at runtime directly (the
    admin socket bypasses the monitor, unlike <command>ceph tell</command>
    <replaceable>daemon-type</replaceable>.<replaceable>id</replaceable>
    injectargs, which relies on the monitor but does not require you to log
    in directly to the host in question).
   </para>
  </sect2>
 </sect1>
 <sect1 xml:id="ceph.pools">
  <title>Managing Storage Pools</title>

<!-- 2014-11-21 tbazant: reusing
 http://ceph.com/docs/master/rados/operations/pools/ -->

  <para>
   When you first deploy a cluster without creating a pool, &ceph; uses the
   default pools for storing data. A pool provides you with:
  </para>

  <itemizedlist mark="bullet" spacing="normal">
   <listitem>
    <para>
     <emphasis>Resilience</emphasis>: You can set how many OSDs are allowed
     to fail without losing data. For replicated pools, it is the desired
     number of copies/replicas of an object. A typical configuration stores
     an object and one additional copy (that is
     <emphasis>size=2</emphasis>), but you can determine the number of
     copies/replicas. For erasure coded pools, it is the number of coding
     chunks (that is <emphasis>m=2</emphasis> in the erasure code profile).
    </para>
   </listitem>
   <listitem>
    <para>
     <emphasis>Placement Groups</emphasis>: You can set the number of
     placement groups for the pool. A typical configuration uses
     approximately 100 placement groups per OSD to provide optimal balancing
     without using up too many computing resources. When setting up multiple
     pools, be careful to ensure you set a reasonable number of placement
     groups for both the pool and the cluster as a whole.
    </para>
   </listitem>
   <listitem>
    <para>
     <emphasis>CRUSH Rules</emphasis>: When you store data in a pool, a
     CRUSH ruleset mapped to the pool enables CRUSH to identify a rule for
     the placement of the object and its replicas (or chunks for erasure
     coded pools) in your cluster. You can create a custom CRUSH rule for
     your pool.
    </para>
   </listitem>
   <listitem>
    <para>
     <emphasis>Snapshots</emphasis>: When you create snapshots with
     <command>ceph osd pool mksnap</command>, you effectively take a
     snapshot of a particular pool.
    </para>
   </listitem>
   <listitem>
    <para>
     <emphasis>Set Ownership</emphasis>: You can set a user ID as the owner
     of a pool.
    </para>
   </listitem>
  </itemizedlist>

  <para>
   To organize data into pools, you can list, create, and remove pools. You
   can also view the usage statistics for each pool.
  </para>

  <sect2 xml:id="ceph.pools.operate">
   <title>Operating Pools</title>
   <para>
    This section introduces practical information to perform basic tasks
    with pools. You can find out how to list, create, and delete pools, as
    well as show pool statistics or manage snapshots of a pool.
   </para>
   <sect3>
    <title>List Pools</title>
    <para>
     To list your cluster’s pools, execute:
    </para>
<screen>ceph osd lspools
0 data,1 metadata,2 rbd,</screen>
    <para>
     The command outputs information about data, metadata, and rados block
     devices (rbd).
    </para>
   </sect3>
   <sect3 xml:id="ceph.pools.operate.add_pool">
    <title>Create a Pool</title>
    <para>
     To create a replicated pool, execute:
    </para>
<screen>ceph osd pool create <replaceable>pool_name</replaceable> <replaceable>pg_num</replaceable> <replaceable>pgp_num</replaceable> <replaceable>pgp_type</replaceable> <replaceable>crush_ruleset_name</replaceable></screen>
    <para>
     To create an erasure pool, execute:
    </para>
<screen>ceph osd pool create <replaceable>pool_name</replaceable> <replaceable>pg_num</replaceable> <replaceable>pgp_num</replaceable> <replaceable>pgp_type</replaceable> <replaceable>erasure_code_profile</replaceable> \
<replaceable>crush_ruleset_name</replaceable></screen>
    <variablelist>
     <varlistentry>
      <term>pool_name</term>
      <listitem>
       <para>
        The name of the pool. It must be unique. This option is required.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>pg_num</term>
      <listitem>
       <para>
        The total number of placement groups for the pool. This option is
        required. Default value is 8.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>pgp_num</term>
      <listitem>
       <para>
        The total number of placement groups for placement purposes. This
        should be equal to the total number of placement groups, except for
        placement group splitting scenarios. This option is required.
        Default value is 8.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>pgp_type</term>
      <listitem>
       <para>
        The pool type which may either be <emphasis>replicated</emphasis> to
        recover from lost OSDs by keeping multiple copies of the objects or
        <emphasis>erasure</emphasis> to get a kind of generalized RAID5
        capability. The replicated pools require more raw storage but
        implement all &ceph; operations. The erasure pools require less raw
        storage but only implement a subset of the available operations.
        Default is 'replicated'.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>crush_ruleset_name</term>
      <listitem>
       <para>
        The name of the crush ruleset for this pool. If the specified
        ruleset does not exist, the creation of replicated pool will fail
        with -ENOENT. But replicated pool will create a new erasure ruleset
        with specified name. The default value is 'erasure-code' for erasure
        pool. Picks up &ceph; configuration variable
        <option>osd_pool_default_crush_replicated_ruleset</option> for
        replicated pool.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>erasure_code_profile=profile</term>
      <listitem>
       <para>
        For erasure pools only. Use the erasure code profile. It must be an
        existing profile as defined by <command>osd erasure-code-profile
        set</command>.
       </para>
      </listitem>
     </varlistentry>
    </variablelist>
    <para>
     When you create a pool, set the number of placement groups to a
     reasonable value (for example 100). Consider the total number of
     placement groups per OSD too. Placement groups are computationally
     expensive, so performance will degrade when you have many pools with
     many placement groups (for example 50 pools with 100 placement groups
     each). The point of diminishing returns depends upon the power of the
     OSD host.
    </para>
    <para>
     See
     <link xlink:href="http://docs.ceph.com/docs/master/rados/operations/placement-groups/">Placement
     Groups</link> for details on calculating an appropriate number of
     placement groups for your pool.
    </para>
   </sect3>
   <sect3>
    <title>Set Pool Quotas</title>
    <para>
     You can set pool quotas for the maximum number of bytes and/or the
     maximum number of objects per pool.
    </para>
<screen>ceph osd pool set-quota <replaceable>pool-name</replaceable> <replaceable>max_objects</replaceable> <replaceable>obj-count</replaceable> <replaceable>max_bytes</replaceable> <replaceable>bytes</replaceable></screen>
    <para>
     For example:
    </para>
<screen>ceph osd pool set-quota data max_objects 10000</screen>
    <para>
     To remove a quota, set its value to 0.
    </para>
   </sect3>
   <sect3 xml:id="ceph.pools.operate.del_pool">
    <title>Delete a Pool</title>
    <para>
     To delete a pool, execute:
    </para>
<screen>ceph osd pool delete <replaceable>pool-name</replaceable> <replaceable>pool-name</replaceable> --yes-i-really-really-mean-it</screen>
    <para>
     If you created your own rulesets and rules for a pool you created, you
     should consider removing them when you no longer need your pool. If you
     created users with permissions strictly for a pool that no longer
     exists, you should consider deleting those users too.
    </para>
   </sect3>
   <sect3>
    <title>Rename a Pool</title>
    <para>
     To rename a pool, execute:
    </para>
<screen>ceph osd pool rename <replaceable>current-pool-name</replaceable> <replaceable>new-pool-name</replaceable></screen>
    <para>
     If you rename a pool and you have per-pool capabilities for an
     authenticated user, you must update the user’s capabilities with the
     new pool name.
    </para>
   </sect3>
   <sect3>
    <title>Show Pool Statistics</title>
    <para>
     To show a pool’s usage statistics, execute:
    </para>
<screen>rados df
pool name  category  KB  objects   lones  degraded  unfound  rd  rd KB  wr  wr KB
cold-storage    -   228   1         0      0          0       0   0      1   228
data            -    1    4         0      0          0       0   0      4    4
hot-storage     -    1    2         0      0          0       15  10     5   231
metadata        -    0    0         0      0          0       0   0      0    0
pool1           -    0    0         0      0          0       0   0      0    0
rbd             -    0    0         0      0          0       0   0      0    0
total used          266268          7
total avail       27966296
total space       28232564</screen>
   </sect3>
   <sect3>
    <title>Make a Snapshot of a Pool</title>
    <para>
     To make a snapshot of a pool, execute:
    </para>
<screen>ceph osd pool mksnap <replaceable>pool-name</replaceable> <replaceable>snap-name</replaceable></screen>
    <para>
     For example:
    </para>
<screen>ceph osd pool mksnap pool1 snapshot1
created pool pool1 snap snapshot1</screen>
   </sect3>
   <sect3>
    <title>Remove a Snapshot of a Pool</title>
    <para>
     To remove a snapshot of a pool, execute:
    </para>
<screen>ceph osd pool rmsnap <replaceable>pool-name</replaceable> <replaceable>snap-name</replaceable></screen>
   </sect3>
   <sect3 xml:id="ceph.pools.values">
    <title>Set Pool Values</title>
    <para>
     To set a value to a pool, execute:
    </para>
<screen>ceph osd pool set <replaceable>pool-name</replaceable> <replaceable>key</replaceable> <replaceable>value</replaceable></screen>
    <para>
     You may set values for the following keys:
    </para>
    <variablelist>
     <varlistentry>
      <term>size</term>
      <listitem>
       <para>
        Sets the number of replicas for objects in the pool. See
        <xref linkend="ceph.pools.options.num_of_replicas"/> for further
        details. Replicated pools only.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>min_size</term>
      <listitem>
       <para>
        Sets the minimum number of replicas required for I/O. See
        <xref linkend="ceph.pools.options.num_of_replicas"/> for further
        details. Replicated pools only.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>crash_replay_interval</term>
      <listitem>
       <para>
        The number of seconds to allow clients to replay acknowledged, but
        uncommitted requests.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>pg_num</term>
      <listitem>
       <para>
        The number of placement groups for the pool.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>pgp_num</term>
      <listitem>
       <para>
        The effective number of placement groups to use when calculating
        data placement.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>crush_ruleset</term>
      <listitem>
       <para>
        The ruleset to use for mapping object placement in the cluster.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>hashpspool</term>
      <listitem>
       <para>
        Set/Unset HASHPSPOOL flag on a given pool. 1 sets flag, 0 unsets
        flag.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>hit_set_type</term>
      <listitem>
       <para>
        Enables hit set tracking for cache pools. See
        <link xlink:href="http://en.wikipedia.org/wiki/Bloom_filter">Bloom
        Filter</link> for additional information. This option can have the
        following values: <literal>bloom</literal>,
        <literal>explicit_hash</literal>,
        <literal>explicit_object</literal>. Default is
        <literal>bloom</literal>.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>hit_set_count</term>
      <listitem>
       <para>
        The number of hit sets to store for cache pools. The higher the
        number, the more RAM consumed by the
        <systemitem>ceph-osd</systemitem> daemon. Default is
        <literal>1</literal>.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>hit_set_period</term>
      <listitem>
       <para>
        The duration of a hit set period in seconds for cache pools. The
        higher the number, the more RAM consumed by the
        <systemitem>ceph-osd</systemitem> daemon.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>hit_set_fpp</term>
      <listitem>
       <para>
        The false positive probability for the bloom hit set type. See
        <link xlink:href="http://en.wikipedia.org/wiki/Bloom_filter">Bloom
        Filter</link> for additional information. Valid range is 0.0 - 1.0
        Default is <literal>0.05</literal>
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>cache_target_dirty_ratio</term>
      <listitem>
       <para>
        The percentage of the cache pool containing modified (dirty) objects
        before the cache tiering agent will flush them to the backing
        storage pool. Default is <literal>.4</literal>
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>cache_target_full_ratio</term>
      <listitem>
       <para>
        The percentage of the cache pool containing unmodified (clean)
        objects before the cache tiering agent will evict them from the
        cache pool. Default is <literal>.8</literal>
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>target_max_bytes</term>
      <listitem>
       <para>
        &ceph; will begin flushing or evicting objects when the
        <option>max_bytes</option> threshold is triggered.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>target_max_objects</term>
      <listitem>
       <para>
        &ceph; will begin flushing or evicting objects when the
        <option>max_objects</option> threshold is triggered.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>cache_min_flush_age</term>
      <listitem>
       <para>
        The time (in seconds) before the cache tiering agent will flush an
        object from the cache pool to the storage pool.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>cache_min_evict_age</term>
      <listitem>
       <para>
        The time (in seconds) before the cache tiering agent will evict an
        object from the cache pool.
       </para>
      </listitem>
     </varlistentry>
    </variablelist>
   </sect3>
   <sect3>
    <title>Get Pool Values</title>
    <para>
     To get a value from a pool, execute:
    </para>
<screen>ceph osd pool get <replaceable>pool-name</replaceable> <replaceable>key</replaceable></screen>
    <para>
     You can get values for keys listed in
     <xref linkend="ceph.pools.values"/> plus the following keys:
    </para>
    <variablelist>
     <varlistentry>
      <term>pg_num</term>
      <listitem>
       <para>
        The number of placement groups for the pool.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>pgp_num</term>
      <listitem>
       <para>
        The effective number of placement groups to use when calculating
        data placement. Valid range is equal to or less than
        <literal>pg_num</literal>.
       </para>
      </listitem>
     </varlistentry>
    </variablelist>
   </sect3>
   <sect3 xml:id="ceph.pools.options.num_of_replicas">
    <title>Set the Number of Object Replicas</title>
    <para>
     To set the number of object replicas on a replicated pool, execute the
     following:
    </para>
<screen>ceph osd pool set <replaceable>poolname</replaceable> size <replaceable>num-replicas</replaceable></screen>
    <important>
     <para>
      The <replaceable>num-replicas</replaceable> includes the object
      itself. If you for example want the object and two copies of the
      object for a total of three instances of the object, specify 3.
     </para>
    </important>
    <para>
     For example:
    </para>
<screen>ceph osd pool set data size 3</screen>
    <para>
     You may execute this command for each pool.
    </para>
    <note>
     <para>
      An object might accept I/Os in degraded mode with fewer than
      <literal>pool size</literal> replicas. To set a minimum number of
      required replicas for I/O, you should use the
      <literal>min_size</literal> setting. For example:
     </para>
<screen>ceph osd pool set data min_size 2</screen>
     <para>
      This ensures that no object in the data pool will receive I/O with
      fewer than <literal>min_size</literal> replicas.
     </para>
    </note>
   </sect3>
   <sect3>
    <title>Get the Number of Object Replicas</title>
    <para>
     To get the number of object replicas, execute the following:
    </para>
<screen>ceph osd dump | grep 'replicated size'</screen>
    <para>
     &ceph; will list the pools, with the <literal>replicated size</literal>
     attribute highlighted. By default, &ceph; creates two replicas of an
     object (a total of three copies, or a size of 3).
    </para>
   </sect3>
  </sect2>
 </sect1>
</chapter>
