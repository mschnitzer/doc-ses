<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<!-- Converted by suse-upgrade version 1.1 -->
<!-- influenced by http://ceph.com/docs/master/install/upgrading-ceph/ -->
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="cha.ceph.upgrade">
 <title>Upgrading from Previous Releases</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:maintainer>tbazant@suse.com</dm:maintainer>
   <dm:status>editing</dm:status>
   <dm:deadline></dm:deadline>
   <dm:priority></dm:priority>
   <dm:translation></dm:translation>
   <dm:languages></dm:languages>
   <dm:release>SES2.1</dm:release>
  </dm:docmanager>
 </info>
 <para>
  This chapter introduces steps to upgrade &storage; from the previous
  release to the current one.
 </para>
 <sect1 xml:id="ceph.upgrade.general">
  <title>General Upgrade Procedure</title>

  <para>
   Before upgrading the &ceph; cluster itself, you need to add a new version
   of the &storage; product to the existing operating system, and update the
   software to the latest versions on each cluster node. You can upgrade
   daemons in your cluster while the cluster is online and in service.
   Certain types of daemons depend upon others. For example &ceph; &rgw;s
   depend upon &ceph; monitors and &ceph; OSD daemons. We recommend
   upgrading in this order:
  </para>

  <orderedlist spacing="normal">
   <listitem>
    <para>
     &ceph; monitors.
    </para>
   </listitem>
   <listitem>
    <para>
     &ceph; OSD daemons.
    </para>
   </listitem>
   <listitem>
    <para>
     &ceph; &rgw;s.
    </para>
   </listitem>
  </orderedlist>

  <tip>
   <para>
    We recommend upgrading all the daemons of a specific type&mdash;for
    example all monitor daemons or all OSD daemons&mdash;one by one to
    ensure that they are all on the same release. We also recommend that you
    upgrade all the daemons in your cluster before you try to exercise new
    functionality in a release.
   </para>
   <para>
    After all the daemons of a specific type are upgraded, check their
    status.
   </para>
   <para>
    Ensure each monitor has rejoined the quorum after all monitors are
    upgraded:
   </para>
<screen>ceph mon stat</screen>
   <para>
    Ensure each &ceph; OSD daemon has rejoined the cluster after all OSDs
    are upgraded:
   </para>
<screen>ceph osd stat</screen>
  </tip>

  <procedure>
   <title>General Upgrade Steps</title>
   <step>
    <para>
     Check that the latest patches are applied to the installed &sls;.
    </para>
<screen>sudo zypper ref &amp;&amp; sudo zypper patch</screen>
   </step>
   <step>
    <para>
     Install &storage; extension to which you are upgrading with
     <menuchoice><guimenu>&yast;</guimenu><guimenu>Software</guimenu><guimenu>Add-On
     Products</guimenu><guimenu>Add</guimenu></menuchoice>.
    </para>
   </step>
   <step>
    <para>
     After the new product is added, refresh the installed software
     repositories.
    </para>
<screen>sudo zypper ref</screen>
   </step>
   <step>
    <para>
     Upgrade the installed software to the new version.
    </para>
<screen>sudo zypper dup</screen>
   </step>
   <step>
    <para>
     After the last &ceph; node is upgraded, check the cluster status.
    </para>
<screen>ceph health</screen>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="ceph.upgrade.2.1to3">
  <title>Upgrade from &storage; 2.1 to 3</title>

  <para>
   This section includes steps specific to upgrading &storage; version 2.1
   to 3.
  </para>

  <para>
   The basic procedure for upgrading from &storage; 2.1 to &storage; 3 is to
   install the <systemitem>ses-upgrade-helper</systemitem> package and run
   the <command>upgrade-to-ses3.sh</command> script as &rootuser; on each
   cluster node (except the admin node) in succession. This can be done
   while the cluster is running. The upgrade involves a number of caveats
   that are described below.
  </para>

  <sect2 xml:id="ceph.upgrade.2.1to3.ceph_uid">
   <title>'ceph' User and Group</title>
   <para>
    As of &storage; 3, the &ceph; daemons (MON, OSD, MDS, RGW) newly run as
    the unprivileged user <systemitem class="username">ceph</systemitem>.
    This is a major change from all previous versions, when all of the
    daemons (except RGW, which was a special case) ran as &rootuser;.
   </para>
   <para>
    Since the <systemitem class="username">ceph</systemitem> user will
    probably already be present on all your nodes, you need to either delete
    it or rename it to something else before installing &storage; 3. We
    recommend renaming it to
    <systemitem class="username">cephadm</systemitem> on all nodes including
    the admin node. On the cluster nodes, the
    <command>upgrade-to-ses3.sh</command> script takes care of this for you.
    On the admin node, it can be done manually:
   </para>
<screen>sudo usermod -l cephadm ceph</screen>
   <para>
    During the product upgrade, a new
    <systemitem class="username">ceph</systemitem> user and group are
    created with the default UID/GID 167 (if available). If UID/GID 167 are
    not available in your environment, the first available UID/GID will be
    used when creating the <systemitem class="username">ceph</systemitem>
    user and group. Since this can complicate migration of OSD disks from
    one node to another, it is good practice to choose (and impose) a
    cluster-wide UID/GID for the
    <systemitem class="username">ceph</systemitem> user and group. But this
    is only necessary if 167, the preferred UID/GID, is already used for
    some other user/group.
   </para>
  </sect2>

  <sect2 xml:id="ceph.upgrade.2.1to3.varlibceph">
   <title>Change Ownership of <filename>/var/lib/ceph</filename></title>
   <para>
    The &ceph; daemons store their state in
    <filename>/var/lib/ceph</filename>. In &storage; version 1, 2 and 2.1
    the daemons run as &rootuser;, and the
    <filename>/var/lib/ceph</filename> directory is owned by &rootuser;.
    Upgrading from any of these versions to &storage; 3 requires manually
    changing the ownership to ensure that the upgraded daemons can access
    this directory tree.
   </para>
   <para>
    Before upgrading the &ceph; related packages, make sure that
    <option>CEPH_AUTO_RESTART_ON_UPGRADE</option> is set to "no" in
    <filename>/etc/sysconfig/ceph</filename>.
   </para>
   <para>
    After upgrading the &ceph; related packages, do <emphasis>not</emphasis>
    restart <systemitem>ceph.target</systemitem>. Instead, follow these
    steps on each node:
   </para>
   <procedure>
    <step>
     <para>
      Stop &storage; 2.1 daemons:
     </para>
<screen>sudo systemctl stop ceph.target</screen>
    </step>
    <step>
     <para>
      Change the ownership of <filename>/var/lib/ceph</filename>:
     </para>
<screen>sudo chown -R ceph:ceph /var/lib/ceph</screen>
    </step>
    <step>
     <para>
      Start &storage; 3 daemons:
     </para>
<screen>sudo systemctl start ceph.target</screen>
    </step>
   </procedure>
   <tip>
    <para>
     You do not need to perform this step manually if you are using the
     <command>upgrade-to-ses3.sh</command> script from the
     <systemitem>ses-upgrade-helper</systemitem> package.
    </para>
   </tip>
  </sect2>

  <sect2 xml:id="ceph.upgrade.2.1to3.rgw">
   <title>&rgw; Instance Name Change</title>
   <para>
    In &storage; 2.1 and earlier, <command>ceph-deploy</command> prefixed
    &rgw; instance names with the string <literal>radosgw.</literal> and
    this prefix was hard-coded in the &rgw; &systemd; unit file.
   </para>
   <para>
    As of &storage; 3, this prefix is no longer hard-coded in the unit file.
   </para>
   <para>
    To ensure a smooth upgrade, it is necessary to disable the &rgw; service
    (using the instance name without the prefix) before the upgrade and
    re-enable it using the full instance name after the upgrade. In other
    words, before upgrading your &rgw; nodes, do the following:
   </para>
<screen>systemctl disable ceph-radosgw@instance_name.service</screen>
   <para>
    And after the upgrade, do:
   </para>
<screen>sudo systemctl enable ceph-radosgw@radosgw.instance_name.service</screen>
   <para>
    If you are using the <command>upgrade-to-ses3.sh</command> script from
    the <systemitem>ses-upgrade-helper</systemitem> package (recommended),
    this is taken care of. The script parses <filename>ceph.conf</filename>
    and runs these pre- and post-commands for you.
   </para>
  </sect2>

  <sect2 xml:id="ceph.upgrade.2.1to3.rgwlog">
   <title>&rgw; Log File</title>
   <para>
    In &storage; 2.1, each &rgw; instance has a section named
    <literal>[client.radosgw.<replaceable>instance_name</replaceable>]</literal>
    in <filename>/etc/ceph/ceph.conf</filename>, where
    <replaceable>instance_name</replaceable> is the name of your &rgw;
    instance. This section contains &rgw;-specific configuration parameters.
    One of these parameters is
   </para>
<screen>log_file = /var/log/ceph-radosgw/ceph.client.radosgw.<replaceable>instance_name</replaceable>.log</screen>
   <para>
    In &storage; 3, &rgw; no longer logs to
    <filename>/var/log/ceph-radosgw</filename>. Instead, the log file with
    the same name is expected to be in <filename>/var/log/ceph</filename>
    along with the log files of the other &ceph; daemons.
   </para>
   <para>
    The <command>upgrade-to-ses3.sh</command> script removes this line from
    <filename>/etc/ceph/ceph.conf</filename> if it is present. If you are
    upgrading manually, you will need to do this yourself.
   </para>
   <para>
    After the upgrade, &rgw; instances will log to
    <filename>/var/log/ceph</filename>, along with the other &ceph; daemons.
    Their log files will be rotated by the standard &ceph; logrotate
    configuration file <filename>/etc/logrotate.d/ceph</filename>.
   </para>
  </sect2>

  <sect2 xml:id="ceph.upgrade.2.1to3.tunables">
   <title>CRUSH Tunables</title>
   <para>
    During the upgrade, you may see the cluster go into "HEALTH_WARN" state
    and complain about "legacy tunables". This is not dangerous, and can be
    rectified after the entire cluster is running &storage; 3.
   </para>
   <para>
    If your cluster is still complaining about "legacy tunables" after all
    nodes have been upgraded, you have three options:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      Option 1: suppress the warning
     </para>
     <para>
      You can stay on legacy tunables and suppress the warning by adding the
      following option to your <filename>ceph.conf</filename> [mon] section:
     </para>
<screen>mon warn on legacy crush tunables = false</screen>
     <para>
      For the change to take effect, you will need to restart the monitors,
      or apply the option to running monitors with:
     </para>
<screen>sudo ceph tell mon.\* injectargs --no-mon-warn-on-legacy-crush-tunables</screen>
    </listitem>
    <listitem>
     <para>
      Option 2: conservative tunables setting
     </para>
     <para>
      Adjusting the tunables will result in some data movement, which can
      affect cluster performance. Choose a time when the cluster is not
      under heavy load, and run:
     </para>
<screen>sudo ceph osd crush tunables firefly</screen>
     <para>
      Although the "firefly" tunables profile may not be the 'latest and
      greatest', some supported clients and kernels may not be able to work
      with the newer tunables.
     </para>
    </listitem>
    <listitem>
     <para>
      Option 3: optimal tunables setting
     </para>
     <warning>
      <para>
       Note that this is not recommended or supported and you take such a
       step at your own risk.
      </para>
     </warning>
     <para>
      If you are certain that all your clients and kernels support the
      latest tunables, you can migrate the cluster to them by running:
     </para>
<screen>sudo ceph osd crush tunables optimal</screen>
     <tip>
      <para>
       Since adjusting CRUSH tunables can cause significant data movement
       within the cluster, consider doing it when the cluster's load is low.
      </para>
     </tip>
    </listitem>
   </itemizedlist>
  </sect2>

  <sect2 xml:id="ceph.upgrade.2.1to3.calamari">
   <title>Calamari Server Upgrade</title>
   <para>
    To upgrade the Calamari server, follow these steps:
   </para>
   <procedure>
    <step>
     <para>
      Stop &smaster; on the node where Calamari is installed:
     </para>
<screen>sudo rcsalt-master stop</screen>
    </step>
    <step>
     <para>
      Check that there are no stale <systemitem>salt-master</systemitem>
      processes running:
     </para>
<screen>ps -e | grep salt-master</screen>
    </step>
    <step>
     <para>
      Start &smaster;:
     </para>
<screen>sudo rcsalt-master start</screen>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="ceph.upgrade.2.1to3.iscsi">
   <title>Updated Behavior for iSCSI Gateway</title>
   <para>
    The <option>rbd_name</option> is a backward compatibility option for
    setting the backstore name to only use the name of the image. Starting
    from &storage; 3, the default uses
   </para>
<screen><replaceable>pool_name</replaceable>-<replaceable>image_name</replaceable></screen>
   <para>
    Do not use this option for new installations.
   </para>
<screen>"pools": [
    {
        "pool": "rbd",
        "gateways": [
            {
                "host": "igw1", 
                "tpg": [
                    {
                        "image": "archive",
                        "rbd_name": "simple"
                    }
                ]
            } 
        ] 
    }
]</screen>
   <para>
    Likewise, <option>wwn_generate</option> will use the original scheme of
    target and image name for setting the <option>vpn_unit_serial</option>.
    The current default uses
   </para>
<screen><replaceable>pool_name</replaceable>-<replaceable>target</replaceable>-<replaceable>image_name</replaceable></screen>
   <para>
    Do not use this option for new installations.
   </para>
<screen>"targets": [
  {
    "host": "igw1",
    "target": "iqn.2003-01.org.linux-iscsi.generic.x86:sn.abcdefghijk",
    "wwn_generate": "original"
  }</screen>
   <para>
    For more information, see
    <filename>/usr/share/doc/lrbd/README.migration</filename>.
   </para>
  </sect2>
 </sect1>
 <sect1 xml:id="ceph.upgrade.1to2">
  <title>Upgrade from &storage; 1.0 to 2</title>

  <para>
   This section includes steps specific to upgrading &storage; version 1.0
   to 2.
  </para>

  <sect2 xml:id="ceph.upgrade.apache2civetweb">
   <title>Apache to Civetweb Migration</title>
   <para>
    While &storage; 1.0 used Apache as an application server for &rgw;
    client/server communication, &storage; 2 introduced a new application
    server called Civetweb. Civetweb is a lightweight embedded Web server,
    which makes &rgw; configuration easier.
   </para>
   <para>
    Although &rgw; in &storage; 2 still works well with Apache, support for
    Apache will end in future &storage; releases. You can migrate to
    Civetweb either manually, or using <command>ceph-deploy</command>.
   </para>
   <para>
    To migrate your existing Apache to Civetweb manually, follow these
    steps:
   </para>
   <procedure>
    <step>
     <para>
      Check if Apache is running on your system:
     </para>
<screen>sudo systemctl status apache2.service</screen>
    </step>
    <step>
     <para>
      Stop Apache if it is running:
     </para>
<screen>sudo systemctl stop apache2.service</screen>
    </step>
    <step>
     <para>
      Disable Apache automatic start:
     </para>
<screen>sudo systemctl disable apache2.service</screen>
    </step>
    <step>
     <para>
      Edit <filename>/etc/ceph/ceph.conf</filename> and add the following
      lines:
     </para>
<screen>[client.radosgw.gateway]
rgw frontends = "civetweb port=80"</screen>
    </step>
    <step>
     <para>
      Restart &rgw;:
     </para>
<screen>sudo systemctl restart ceph-radosgw@<replaceable>gateway_name</replaceable>.service</screen>
    </step>
   </procedure>
   <para>
    To migrate your existing Apache to Civetweb using
    <command>ceph-deploy</command>, follow these steps:
   </para>
   <procedure>
    <step>
     <para>
      List &rgw; instances within the &ceph; cluster to get the instance
      name:
     </para>
<screen>ceph-deploy --overwrite-conf rgw list
node1:rgw1</screen>
    </step>
    <step>
     <para>
      Delete the existing &rgw; instance you want to migrate:
     </para>
<screen>ceph-deploy --overwrite-conf rgw delete node1:rgw1</screen>
    </step>
    <step>
     <para>
      Create a new CivetWeb based &rgw; instance:
     </para>
<screen>ceph-deploy --overwrite-conf rgw create node1:rgw1</screen>
     <tip>
      <para>
       If you still need to deploy Apache based &rgw; instead of the
       recommended CivetWeb, add the <option>--cgi</option> option:
      </para>
<screen>ceph-deploy --overwrite-conf rgw --cgi create node1:rgw1</screen>
      <note>
       <para>
        Apache listens on port 80 by default. If you need to deploy it on a
        different port, you need to manually cancel listening on the default
        port 80. You can do it by editing the
        <filename>/etc/apache2/listen.conf</filename> file, commenting the
        line that contains <literal>Listen 80</literal>, and restarting the
        service:
       </para>
<screen>sudo systemctl restart ceph-radosgw@<replaceable>gateway</replaceable>.service</screen>
      </note>
     </tip>
    </step>
   </procedure>
  </sect2>
 </sect1>
</chapter>
